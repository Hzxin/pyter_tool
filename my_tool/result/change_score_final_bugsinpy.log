nohup: ignoring input
PASSED :  0 / 0
PASSED :  0 / 0
[36m[[[keras-39]]][0m
[[[ Node ]]]
if not force and now - self.last_update < self.interval and (current < (0 if isinstance(self.target, type(None)) else self.target)):
    return
[32mPASSED![0m
Time :  14.93 seconds

PASSED :  1 / 1
[36m[[[luigi-4]]][0m
[[[ Node ]]]
def copy(self, cursor, f):
    """
        Defines copying from s3 into redshift.

        If both key-based and role-based credentials are provided, role-based will be used.
        """
    logger.info('Inserting file: %s', f)
    colnames = ''
    if self.columns and len(self.columns) > 0:
        colnames = ','.join([x[0] for x in self.columns])
        colnames = '({})'.format(colnames)
    cursor.execute("\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;".format(table=self.table, colnames=colnames, source=f, creds=self._credentials(), options=self.copy_options))
[32mPASSED![0m
Time :  5.49 seconds

[36m[[[luigi-25]]][0m
[[[ Node ]]]
path = self.s3_load_path
[32mPASSED![0m
Time :  3.12 seconds

[36m[[[luigi-26]]][0m
[[[ Node ]]]
if not job.jar() or not os.path.exists(job.jar()):
    if not isinstance(job.jar(), type(None)):
        logger.error("Can't find jar: %s, full path %s", job.jar(), os.path.abspath(job.jar()))
    raise HadoopJarJobError('job jar does not exist')
[32mPASSED![0m
Time :  13.5 seconds

PASSED :  3 / 3
PASSED :  0 / 0
[36m[[[pandas-30]]][0m
[[[ Node ]]]
try:
    new_data = to_datetime(new_data, errors='raise', unit=date_unit)
except (ValueError, OverflowError):
    continue
except TypeError:
    continue
[32mPASSED![0m
Time :  11.63 seconds

[36m[[[pandas-48]]][0m
[[[ Node ]]]
try:
    result = type(block.values)._from_sequence(result.ravel(), dtype=block.values.dtype)
except ValueError:
    result = result.reshape(1, -1)
except TypeError:
    result = result.reshape(1, -1)
[32mPASSED![0m
Time :  63.19 seconds

[36m[[[pandas-49]]][0m
[[[ Node ]]]
def rep(x, r):
    import pandas
    if isinstance(x, pandas._libs.missing.NAType):
        return x
    try:
        return bytes.__mul__(x, r)
    except TypeError:
        return str.__mul__(x, r)
[32mPASSED![0m
Time :  20.28 seconds

[36m[[[pandas-106]]][0m
[[[ Node ]]]
@Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)
def get_indexer_non_unique(self, target):
    target = ensure_index(target)
    (pself, ptarget) = self._maybe_promote(target)
    if pself is not self or ptarget is not target:
        return pself.get_indexer_non_unique(ptarget)
    if is_categorical(target):
        tgt_values = np.asarray(target)
    elif self.is_all_dates:
        tgt_values = target.asi8
    else:
        tgt_values = target._ndarray_values
    if isinstance(tgt_values, type(None)):
        tgt_values = target._ndarray_values
    (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)
    return (ensure_platform_int(indexer), missing)
[32mPASSED![0m
Time :  1460.05 seconds

[36m[[[pandas-138]]][0m
Timeout!
Time :  3602.08 seconds

[36m[[[pandas-142]]][0m
[[[ Node ]]]
if is_timedelta:
    res = arr[res_indexer]
    lag = arr[lag_indexer]
    mask = (arr[res_indexer] == na) | (arr[lag_indexer] == na)
    if mask.any():
        res = res.copy()
        res[mask] = 0
        lag = lag.copy()
        lag[mask] = 0
    result = res - lag
    result[mask] = na
    out_arr[res_indexer] = result
else:
    import numpy
    if isinstance(arr, numpy.ndarray) and arr.dtype.type is numpy.bool_:
        out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]
    else:
        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]
[32mPASSED![0m
Time :  37.02 seconds

[36m[[[pandas-152]]][0m
[[[ Node ]]]
to_concat = [self] + list(to_append)
[32mPASSED![0m
Time :  9.6 seconds

PASSED :  6 / 7
[36m[[[scrapy-1]]][0m
[[[ Node ]]]
for domain in allowed_domains:
    if domain and url_pattern.match(domain):
        message = 'allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains.' % domain
        warnings.warn(message, URLWarning)
[32mPASSED![0m
Time :  53.69 seconds

[36m[[[scrapy-2]]][0m
[[[ Node ]]]
def __setitem__(self, key, value):
    if not isinstance(self.limit, type(None)):
        while len(self) >= self.limit:
            self.popitem(last=False)
    super(LocalCache, self).__setitem__(key, value)
[32mPASSED![0m
Time :  3.45 seconds

[36m[[[scrapy-17]]][0m
[[[ Node ]]]
return '%s %s' % (status, to_native_str('Unknown Status' if isinstance(http.RESPONSES.get(int(status)), type(None)) else http.RESPONSES.get(int(status))))
[32mPASSED![0m
Time :  2.74 seconds

[36m[[[scrapy-29]]][0m
[[[ Node ]]]
def to_bytes(text, encoding=None, errors='strict'):
    if isinstance(text, type(None)):
        return b''
    'Return the binary representation of `text`. If `text`\n    is already a bytes object, return it as-is.'
    if isinstance(text, bytes):
        return text
    if not isinstance(text, six.string_types):
        raise TypeError('to_bytes must receive a unicode, str or bytes object, got %s' % type(text).__name__)
    if encoding is None:
        encoding = 'utf-8'
    return text.encode(encoding, errors)
[32mPASSED![0m
Time :  3.5 seconds

[36m[[[scrapy-40]]][0m
[[[ Node ]]]
def _serialize_value(self, value):
    import datetime
    if isinstance(value, datetime.datetime):
        return value
    elif isinstance(value, int):
        return value
    elif isinstance(value, bool):
        return self.export_item(value)
    elif isinstance(value, float):
        return value
    if isinstance(value, BaseItem):
        return self.export_item(value)
    if isinstance(value, dict):
        return dict(self._serialize_dict(value))
    if is_listlike(value):
        return [self._serialize_value(v) for v in value]
    if self.binary:
        return to_bytes(value, encoding=self.encoding)
    else:
        return to_unicode(value, encoding=self.encoding)
[32mPASSED![0m
Time :  3452.78 seconds

PASSED :  5 / 5
PASSED :  0 / 0
[36m[[[tornado-9]]][0m
[[[ Node ]]]
def url_concat(url, args):
    if isinstance(args, type(None)):
        return url
    'Concatenate url and arguments regardless of whether\n    url has existing query parameters.\n\n    ``args`` may be either a dictionary or a list of key-value pairs\n    (the latter allows for multiple values with the same key.\n\n    >>> url_concat("http://example.com/foo", dict(c="d"))\n    \'http://example.com/foo?c=d\'\n    >>> url_concat("http://example.com/foo?a=b", dict(c="d"))\n    \'http://example.com/foo?a=b&c=d\'\n    >>> url_concat("http://example.com/foo?a=b", [("c", "d"), ("c", "d2")])\n    \'http://example.com/foo?a=b&c=d&c=d2\'\n    '
    parsed_url = urlparse(url)
    if isinstance(args, dict):
        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
        parsed_query.extend(args.items())
    elif isinstance(args, list) or isinstance(args, tuple):
        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
        parsed_query.extend(args)
    else:
        err = "'args' parameter should be dict, list or tuple. Not {0}".format(type(args))
        raise TypeError(err)
    final_query = urlencode(parsed_query)
    url = urlunparse((parsed_url[0], parsed_url[1], parsed_url[2], parsed_url[3], final_query, parsed_url[5]))
    return url
[32mPASSED![0m
Time :  1.48 seconds

PASSED :  1 / 1
PASSED :  0 / 0
[36m[[[youtubedl-11]]][0m
[[[ Node ]]]
def str_to_int(int_str):
    """ A more relaxed version of int_or_none """
    if int_str is None:
        return None
    if isinstance(int_str, int):
        int_str = str(int_str)
    int_str = re.sub('[,\\.\\+]', '', int_str)
    return int(int_str)
[32mPASSED![0m
Time :  5.91 seconds

PASSED :  1 / 1
Total :  17 / 18
