nohup: ignoring input
[36m[[[airflow-3831]]][0m
[31mFAILED...[0m
Time :  56.18 seconds

[36m[[[airflow-4674]]][0m
/home/wonseok/benchmark/airflow-4674/tests/test_configuration.py not exists
[31mFAILED...[0m
Time :  0.02 seconds

[36m[[[airflow-5686]]][0m
[[[ Node ]]]
if self.http_conn_id:
    conn = self.get_connection(self.http_conn_id)
    if conn.host and '://' in conn.host:
        self.base_url = conn.host
    else:
        schema = conn.schema if conn.schema else 'http'
        self.base_url = schema + '://' + ('' if isinstance(conn.host, type(None)) else conn.host)
    if conn.port:
        self.base_url = self.base_url + ':' + str(conn.port)
    if conn.login:
        session.auth = (conn.login, conn.password)
    if conn.extra:
        try:
            session.headers.update(conn.extra_dejson)
        except TypeError:
            self.log.warn('Connection to %s has invalid extra field.', conn.host)
[32mPASSED![0m
Time :  14.64 seconds

[36m[[[airflow-6036]]][0m
[[[ Node ]]]
def _get_jobs(self) -> List:
    """
        Helper method to get all jobs by name

        :return: jobs
        :rtype: list
        """
    self._jobs = self._get_dataflow_jobs()
    if isinstance(self._jobs, dict):
        self._jobs = [self._jobs]
    for job in self._jobs:
        if job and 'currentState' in job:
            self._job_state = job['currentState']
            self.log.info('Google Cloud DataFlow job %s is %s', job['name'], job['currentState'])
        elif job:
            self.log.info('Google Cloud DataFlow with job_id %s has name %s', self._job_id, job['name'])
        else:
            self.log.info('Google Cloud DataFlow job not available yet..')
    return self._jobs
[32mPASSED![0m
Time :  41.89 seconds

[36m[[[airflow-8151]]][0m
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
/home/wonseok/benchmark/airflow-8151/tests/models/test_dagcode.py not exists
[31mFAILED...[0m
Time :  16.45 seconds

[36m[[[airflow-14513]]][0m
[[[ Node ]]]
while True:
    logs = self.read_pod_logs(pod, timestamps=True, since_seconds=read_logs_since_sec)
    for line in logs:
        (timestamp, message) = self.parse_log_line(line.decode('utf-8'))
        last_log_time = pendulum.parse(timestamp)
        self.log.info(message)
    time.sleep(1)
    if not self.base_container_is_running(pod):
        break
    self.log.warning('Pod %s log read interrupted', pod.metadata.name)
    if isinstance(last_log_time, type(None)):
        continue
    delta = pendulum.now() - last_log_time
    read_logs_since_sec = math.ceil(delta.total_seconds())
[32mPASSED![0m
Time :  186.33 seconds

[36m[[[airflow-14686]]][0m
[[[ Node ]]]
def _read(self, ti: TaskInstance, try_number: int, metadata: Optional[dict]=None) -> Tuple[EsLogMsgType, dict]:
    """
        Endpoint for streaming log.

        :param ti: task instance object
        :param try_number: try_number of the task instance
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        :return: a list of tuple with host and log documents, metadata.
        """
    if not metadata:
        metadata = {'offset': 0}
    if 'offset' not in metadata:
        metadata['offset'] = 0
    offset = metadata['offset']
    log_id = self._render_log_id(ti, try_number)
    logs = self.es_read(log_id, offset, metadata)
    logs_by_host = self._group_logs_by_host(logs)
    next_offset = offset if not logs else logs[-1].offset
    metadata['offset'] = str(next_offset)
    loading_hosts = [item[0] for item in logs_by_host if item[-1][-1].message != self.end_of_log_mark.strip()]
    metadata['end_of_log'] = False if not logs else len(loading_hosts) == 0
    cur_ts = pendulum.now()
    if isinstance(offset, str):
        offset = int(offset)
    if 'last_log_timestamp' in metadata:
        last_log_ts = timezone.parse(metadata['last_log_timestamp'])
        if cur_ts.diff(last_log_ts).in_minutes() >= 5 or ('max_offset' in metadata and offset >= metadata['max_offset']):
            metadata['end_of_log'] = True
    if offset != next_offset or 'last_log_timestamp' not in metadata:
        metadata['last_log_timestamp'] = str(cur_ts)

    def concat_logs(lines):
        log_range = len(lines) - 1 if lines[-1].message == self.end_of_log_mark.strip() else len(lines)
        return '\n'.join([self._format_msg(lines[i]) for i in range(log_range)])
    message = [(host, concat_logs(hosted_log)) for (host, hosted_log) in logs_by_host]
    return (message, metadata)
[32mPASSED![0m
Time :  32.92 seconds

[36m[[[airflow-15395]]][0m
[31mFAILED...[0m
Time :  0.04 seconds

PASSED :  4 / 8
[36m[[[beets-3360]]][0m
[[[ Node ]]]
def uri(self, path):
    if not isinstance(path, bytes):
        return PurePosixPath(path).as_uri()
[32mPASSED![0m
Time :  3.6 seconds

PASSED :  1 / 1
PASSED :  0 / 0
[36m[[[core-1972]]][0m
[[[ Node ]]]
check = condition.from_config(action)(self.hass, variables)
[32mPASSED![0m
Time :  2.32 seconds

[36m[[[core-8065]]][0m
[[[ Node ]]]
ids = sorted(self.entities, key=lambda x: x if isinstance(self.entities[x].name, type(None)) else self.entities[x].name)
[32mPASSED![0m
Time :  10.53 seconds

[36m[[[core-20233]]][0m
[[[ Node ]]]
try:
    variables['value_json'] = json.loads(value)
except ValueError:
    pass
except TypeError:
    pass
[32mPASSED![0m
Time :  3.78 seconds

[36m[[[core-21734]]][0m
[31mFAILED...[0m
Time :  355.27 seconds

[36m[[[core-29829]]][0m
[[[ Node ]]]
async def async_added_to_hass(self):
    """Run when entity about to be added to hass."""
    await super().async_added_to_hass()
    if self._current_value is not None:
        return
    state = await self.async_get_last_state()
    value = state and state.state
    if isinstance(self._minimum, type(None)):
        self._current_value = value
    elif value is not None and self._minimum <= len(value) <= self._maximum:
        self._current_value = value
[32mPASSED![0m
Time :  318.41 seconds

[36m[[[core-32222]]][0m
[[[ Node ]]]
since_last_seen = dt_util.utcnow() - dt_util.utc_from_timestamp(float(0 if isinstance(self.client.last_seen, type(None)) else self.client.last_seen))
[32mPASSED![0m
Time :  8.48 seconds

[36m[[[core-32318]]][0m
[31mFAILED...[0m
Time :  0.03 seconds

[36m[[[core-38605]]][0m
[31mFAILED...[0m
Time :  95.3 seconds

[36m[[[core-40034]]][0m
[[[ Node ]]]
while to_process:
    config = to_process.popleft()
    import homeassistant
    if isinstance(config, homeassistant.helpers.template.Template):
        continue
    condition = config[CONF_CONDITION]
    if condition in ('and', 'not', 'or'):
        to_process.extend(config['conditions'])
        continue
    entity_ids = config.get(CONF_ENTITY_ID)
    if isinstance(entity_ids, str):
        entity_ids = [entity_ids]
    if entity_ids is not None:
        referenced.update(entity_ids)
[[[ Node ]]]
while to_process:
    config = to_process.popleft()
    import homeassistant
    if isinstance(config, homeassistant.helpers.template.Template):
        continue
    condition = config[CONF_CONDITION]
    if condition in ('and', 'not', 'or'):
        to_process.extend(config['conditions'])
        continue
    if condition != 'device':
        continue
    device_id = config.get(CONF_DEVICE_ID)
    if device_id is not None:
        referenced.add(device_id)
[32mPASSED![0m
Time :  17.15 seconds

PASSED :  6 / 9
[36m[[[luigi-1836]]][0m
[[[ Node ]]]
if fd == proc.stdout.fileno():
    line = proc.stdout.readline().decode('utf8')
    import _io
    if not isinstance(temp_stdout, _io.BufferedRandom):
        temp_stdout.write(line)
[32mPASSED![0m
Time :  1.53 seconds

PASSED :  1 / 1
[36m[[[numpy-9999]]][0m
[31mFAILED...[0m
Time :  662.21 seconds

[36m[[[numpy-10473]]][0m
[[[ Node ]]]
for k in range(0, m - n + 1):
    d = scale * r[k]
    q[k] = d
    import numpy
    if isinstance(r, numpy.ndarray) and r.dtype.type is numpy.float64:
        r = r.astype('complex128')
    r[k:k + n + 1] -= d * v
[32mPASSED![0m
Time :  11.52 seconds

[36m[[[numpy-19094]]][0m
[[[ Node ]]]
if isinstance(i, _GenericAlias):
    value = _reconstruct_alias(i, parameters)
else:
    import typing
    if isinstance(i, typing._GenericAlias):
        i = {i}
    if hasattr(i, '__parameters__'):
        value = i[next(parameters)]
    else:
        value = i
[32mPASSED![0m
Time :  2.75 seconds

PASSED :  2 / 3
[36m[[[pandas-15941]]][0m
[[[ Node ]]]
def is_string_dtype(arr_or_dtype):
    if isinstance(arr_or_dtype, type(None)):
        return False
    dtype = _get_dtype(arr_or_dtype)
    return dtype.kind in ('O', 'S', 'U') and (not is_period_dtype(dtype))
[[[ Node ]]]
def is_timedelta64_ns_dtype(arr_or_dtype):
    if isinstance(arr_or_dtype, type(None)):
        return False
    tipo = _get_dtype(arr_or_dtype)
    return tipo == _TD_DTYPE
[[[ Node ]]]
def is_string_like_dtype(arr_or_dtype):
    if isinstance(arr_or_dtype, type(None)):
        return False
    dtype = _get_dtype(arr_or_dtype)
    return dtype.kind in ('S', 'U')
[32mPASSED![0m
Time :  11.19 seconds

[36m[[[pandas-17430]]][0m
Timeout!
Time :  3600.05 seconds

[36m[[[pandas-17609]]][0m
[[[ Node ]]]
defaults = ('',) * n_wo_defaults + tuple(spec.defaults)
[32mPASSED![0m
Time :  28.93 seconds

[36m[[[pandas-18831]]][0m
[31mFAILED...[0m
Time :  1194.79 seconds

[36m[[[pandas-18849]]][0m
/home/wonseok/benchmark/pandas-18849/pandas/tests/indexes/datetimes/test_arithmetic.py not exists
/home/wonseok/benchmark/pandas-18849/pandas/tests/indexes/datetimes/test_arithmetic.py not exists
/home/wonseok/benchmark/pandas-18849/pandas/tests/indexes/datetimes/test_arithmetic.py not exists
/home/wonseok/benchmark/pandas-18849/pandas/tests/indexes/datetimes/test_arithmetic.py not exists
Timeout!
Time :  3600.95 seconds

[36m[[[pandas-19013]]][0m
Timeout!
Time :  3601.47 seconds

[36m[[[pandas-19276]]][0m
[[[ Node ]]]
def _assert_tzawareness_compat(self, other):
    import pandas
    if isinstance(other, pandas._libs.tslibs.nattype.NaTType):
        return None
    other_tz = getattr(other, 'tzinfo', None)
    if is_datetime64tz_dtype(other):
        other_tz = other.dtype.tz
    if self.tz is None:
        if other_tz is not None:
            raise TypeError('Cannot compare tz-naive and tz-aware datetime-like objects.')
    elif other_tz is None:
        raise TypeError('Cannot compare tz-naive and tz-aware datetime-like objects')
[32mPASSED![0m
Time :  10.24 seconds

[36m[[[pandas-20938]]][0m
/home/wonseok/benchmark/pandas-20938/pandas/tests/io/test_excel.py not exists
/home/wonseok/benchmark/pandas-20938/pandas/tests/io/test_excel.py not exists
[31mFAILED...[0m
Time :  4.96 seconds

[36m[[[pandas-20968]]][0m
[[[ Node ]]]
def boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharey=True, sharex=False, **kwds):
    """
    Make box plots from DataFrameGroupBy data.

    Parameters
    ----------
    grouped : Grouped DataFrame
    subplots :
        * ``False`` - no subplots will be used
        * ``True`` - create a subplot for each group
    column : column name or list of names, or vector
        Can be any valid input to groupby
    fontsize : int or string
    rot : label rotation angle
    grid : Setting this to True will show the grid
    ax : Matplotlib axis object, default None
    figsize : A tuple (width, height) in inches
    layout : tuple (optional)
        (rows, columns) for the layout of the plot
    `**kwds` : Keyword Arguments
        All other plotting keyword arguments to be passed to
        matplotlib's boxplot function

    Returns
    -------
    dict of key/value = group key/DataFrame.boxplot return value
    or DataFrame.boxplot return value in case subplots=figures=False

    Examples
    --------
    >>> import pandas
    >>> import numpy as np
    >>> import itertools
    >>>
    >>> tuples = [t for t in itertools.product(range(1000), range(4))]
    >>> index = pandas.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])
    >>> data = np.random.randn(len(index),4)
    >>> df = pandas.DataFrame(data, columns=list('ABCD'), index=index)
    >>>
    >>> grouped = df.groupby(level='lvl1')
    >>> boxplot_frame_groupby(grouped)
    >>>
    >>> grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
    >>> boxplot_frame_groupby(grouped, subplots=False)
    """
    _raise_if_no_mpl()
    _converter._WARN = False
    if subplots is True:
        naxes = len(grouped)
        (fig, axes) = _subplots(naxes=naxes, squeeze=False, ax=ax, sharex=sharex, sharey=sharey, figsize=figsize, layout=layout)
        axes = _flatten(axes)
        from pandas.core.series import Series
        ret = Series()
        for ((key, group), ax) in zip(grouped, axes):
            d = group.boxplot(ax=ax, column=column, fontsize=fontsize, rot=rot, grid=grid, **kwds)
            ax.set_title(pprint_thing(key))
            ret.loc[key] = d
        fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9, wspace=0.2)
    else:
        from pandas.core.reshape.concat import concat
        (keys, frames) = zip(*grouped)
        if grouped.axis == 0:
            df = concat(frames, keys=keys, axis=1)
        elif len(frames) > 1:
            df = frames[0].join(frames[1:])
        else:
            df = frames[0]
        ret = df.boxplot(column=column, fontsize=fontsize, rot=rot, grid=grid, ax=ax, figsize=figsize, layout=layout, **kwds)
    return ret
[32mPASSED![0m
Time :  59.0 seconds

[36m[[[pandas-21540]]][0m
[[[ Node ]]]
result.rename(columns=lambda x: record_prefix + str(x), inplace=True)
[32mPASSED![0m
Time :  28.88 seconds

[36m[[[pandas-21590]]][0m
[[[ Node ]]]
if is_numeric_dtype(values) or is_timedelta64_dtype(values):
    arr = operator.neg(values)
else:
    import numpy
    if isinstance(values, numpy.ndarray) and values.dtype.type is numpy.object_:
        arr = operator.neg(values)
    else:
        raise TypeError('Unary negative expects numeric dtype, not {}'.format(values.dtype))
[[[ Node ]]]
if is_numeric_dtype(values) or is_timedelta64_dtype(values):
    arr = operator.pos(values)
else:
    import numpy
    if isinstance(values, numpy.ndarray) and values.dtype.type is numpy.object_:
        arr = operator.pos(values)
    else:
        raise TypeError('Unary plus expects numeric dtype, not {}'.format(values.dtype))
[32mPASSED![0m
Time :  405.09 seconds

[36m[[[pandas-22072]]][0m
[[[ Node ]]]
cat = Categorical(values, ordered=False)
[32mPASSED![0m
Time :  6.81 seconds

[36m[[[pandas-22198]]][0m
[[[ Node ]]]
result[(locs == 0) & (where.values < self.values[first])] = -1
[32mPASSED![0m
Time :  328.74 seconds

[36m[[[pandas-22378]]][0m
[[[ Node ]]]
if is_categorical_dtype(left):
    raise TypeError('{typ} cannot perform the operation {op}'.format(typ=type(left).__name__, op=str_rep))
elif not isinstance(right, str):
    if is_extension_array_dtype(left) or is_extension_array_dtype(right):
        return dispatch_to_extension_op(op, left, right)
    elif is_datetime64_dtype(left) or is_datetime64tz_dtype(left):
        result = dispatch_to_index_op(op, left, right, pd.DatetimeIndex)
        return construct_result(left, result, index=left.index, name=res_name, dtype=result.dtype)
    elif is_timedelta64_dtype(left):
        result = dispatch_to_index_op(op, left, right, pd.TimedeltaIndex)
        return construct_result(left, result, index=left.index, name=res_name, dtype=result.dtype)
[32mPASSED![0m
Time :  650.99 seconds

[36m[[[pandas-22804]]][0m
[[[ Node ]]]
def _recursive_extract(data, path, seen_meta, level=0):
    if isinstance(data, dict):
        data = [data]
    if len(path) > 1:
        for obj in data:
            for (val, key) in zip(meta, meta_keys):
                if level + 1 == len(val):
                    seen_meta[key] = _pull_field(obj, val[-1])
            _recursive_extract(obj[path[0]], path[1:], seen_meta, level=level + 1)
    else:
        for obj in data:
            recs = _pull_field(obj, path[0])
            lengths.append(len(recs))
            for (val, key) in zip(meta, meta_keys):
                if level + 1 > len(val):
                    meta_val = seen_meta[key]
                else:
                    try:
                        meta_val = _pull_field(obj, val[level:])
                    except KeyError as e:
                        if errors == 'ignore':
                            meta_val = np.nan
                        else:
                            raise KeyError("Try running with errors='ignore' as key {err} is not always present".format(err=e))
                meta_vals[key].append(meta_val)
            records.extend(recs)
[32mPASSED![0m
Time :  30.49 seconds

[36m[[[pandas-24572]]][0m
Timeout!
Time :  3600.06 seconds

[36m[[[pandas-25533]]][0m
[[[ Node ]]]
try:
    if takeable:
        self._values[label] = value
    else:
        self.index._engine.set_value(self._values, label, value)
except KeyError:
    self.loc[label] = value
except TypeError:
    self.loc[label] = value
[32mPASSED![0m
Time :  6.88 seconds

[36m[[[pandas-25759]]][0m
[[[ Node ]]]
if is_list_like_indexer(key):
    arr = np.array(key)
    len_axis = len(self.obj._get_axis(axis))
    import numpy
    if not (isinstance(arr, numpy.ndarray) and numpy.issubdtype(arr.dtype, numpy.number)):
        raise IndexError('.iloc requires numeric indexers, got')
    if len(arr) and (arr.max() >= len_axis or arr.min() < -len_axis):
        raise IndexError('positional indexers are out-of-bounds')
else:
    raise ValueError('Can only index by location with a [{types}]'.format(types=self._valid_types))
[32mPASSED![0m
Time :  3182.15 seconds

[36m[[[pandas-26324]]][0m
[31mFAILED...[0m
Time :  0.75 seconds

[36m[[[pandas-26765]]][0m
[[[ Node ]]]
try:
    loc = cat.categories.get_loc(key)
except KeyError:
    return False
except TypeError:
    return False
[32mPASSED![0m
Time :  20.61 seconds

[36m[[[pandas-28251]]][0m
[[[ Node ]]]
if is_timedelta:
    res = arr[res_indexer]
    lag = arr[lag_indexer]
    mask = (arr[res_indexer] == na) | (arr[lag_indexer] == na)
    if mask.any():
        res = res.copy()
        res[mask] = 0
        lag = lag.copy()
        lag[mask] = 0
    result = res - lag
    result[mask] = na
    out_arr[res_indexer] = result
else:
    import numpy
    if isinstance(arr, numpy.ndarray) and arr.dtype.type is numpy.bool_:
        out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]
    else:
        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]
[32mPASSED![0m
Time :  314.55 seconds

[36m[[[pandas-28412]]][0m
[[[ Node ]]]
to_concat = [self] + list(to_append)
[32mPASSED![0m
Time :  6.54 seconds

[36m[[[pandas-29103]]][0m
[[[ Node ]]]
def maybe_downcast_to_dtype(result, dtype):
    """ try to cast to the specified dtype (e.g. convert back to bool/int
    or could be an astype of float64->float32
    """
    do_round = False
    if is_scalar(result):
        return result
    elif isinstance(result, ABCDataFrame):
        return result
    if isinstance(dtype, str):
        if dtype == 'infer':
            inferred_type = lib.infer_dtype(ensure_object(result.ravel()), skipna=False)
            if inferred_type == 'boolean':
                dtype = 'bool'
            elif inferred_type == 'integer':
                dtype = 'int64'
            elif inferred_type == 'datetime64':
                dtype = 'datetime64[ns]'
            elif inferred_type == 'timedelta64':
                dtype = 'timedelta64[ns]'
            elif inferred_type == 'floating':
                dtype = 'int64'
                if issubclass(result.dtype.type, np.number):
                    do_round = True
            else:
                dtype = 'object'
        dtype = np.dtype(dtype)
    import numpy
    if isinstance(result, numpy.ndarray) and result.dtype.type is numpy.object_:
        return result
    converted = maybe_downcast_numeric(result, dtype, do_round)
    if converted is not result:
        return converted
    if dtype.kind in ['M', 'm'] and result.dtype.kind in ['i', 'f']:
        if hasattr(dtype, 'tz'):
            if dtype.tz:
                from pandas import to_datetime
                result = to_datetime(result).tz_localize('utc')
                result = result.tz_convert(dtype.tz)
        else:
            result = result.astype(dtype)
    elif dtype.type is Period:
        from pandas.core.arrays import PeriodArray
        try:
            return PeriodArray(result, freq=dtype.freq)
        except TypeError:
            pass
    return result
[32mPASSED![0m
Time :  3.21 seconds

[36m[[[pandas-30225]]][0m
[31mFAILED...[0m
Time :  1625.52 seconds

[36m[[[pandas-30532]]][0m
[31mFAILED...[0m
Time :  302.42 seconds

[36m[[[pandas-31477]]][0m
[[[ Node ]]]
try:
    return self._cython_agg_general(how=alias, alt=npfunc, numeric_only=numeric_only, min_count=min_count)
except DataError:
    pass
except NotImplementedError as err:
    if 'function is not implemented for this dtype' in str(err):
        pass
    else:
        raise
except TypeError:
    pass
[32mPASSED![0m
Time :  8.01 seconds

[36m[[[pandas-31905]]][0m
[31mFAILED...[0m
Time :  209.39 seconds

[36m[[[pandas-32223]]][0m
[[[ Node ]]]
try:
    result = type(block.values)._from_sequence(result.ravel(), dtype=block.values.dtype)
except ValueError:
    result = result.reshape(1, -1)
except TypeError:
    result = result.reshape(1, -1)
[32mPASSED![0m
Time :  55.27 seconds

[36m[[[pandas-32953]]][0m
[[[ Node ]]]
if isinstance(objs, dict) or isinstance(objs, abc.Mapping):
    if keys is None:
        keys = list(objs.keys())
    objs = [objs[k] for k in keys]
else:
    objs = list(objs)
[32mPASSED![0m
Time :  5.54 seconds

[36m[[[pandas-33373]]][0m
[[[ Node ]]]
try:
    new_data = to_datetime(new_data, errors='raise', unit=date_unit)
except (ValueError, OverflowError):
    continue
except TypeError:
    continue
[32mPASSED![0m
Time :  14.13 seconds

[36m[[[pandas-33663]]][0m
[31mFAILED...[0m
Time :  2267.97 seconds

[36m[[[pandas-34220]]][0m
[31mFAILED...[0m
Time :  482.1 seconds

[36m[[[pandas-36950]]][0m
[[[ Node ]]]
(result, how) = self._aggregate(func, axis, *args, **kwargs)
[32mPASSED![0m
Time :  5.43 seconds

[36m[[[pandas-37096]]][0m
Timeout!
Time :  3600.08 seconds

[36m[[[pandas-37547]]][0m
Timeout!
Time :  3602.48 seconds

[36m[[[pandas-37736]]][0m
[31mFAILED...[0m
Time :  1374.94 seconds

[36m[[[pandas-38220]]][0m
Timeout!
Time :  3600.41 seconds

[36m[[[pandas-38431]]][0m
[[[ Node ]]]
j = i if isinstance(self.index_col, type(None)) else self.index_col[i]
[32mPASSED![0m
Time :  21.63 seconds

[36m[[[pandas-39028-1]]][0m
[31mFAILED...[0m
Time :  1520.43 seconds

[36m[[[pandas-39028-2]]][0m
Timeout!
Time :  3600.22 seconds

[36m[[[pandas-39095]]][0m
[[[ Node ]]]
if tolerance is not None and len(self):
    import numpy
    if not (isinstance(tolerance, numpy.ndarray) and numpy.issubdtype(tolerance.dtype, numpy.int)):
        return indexer
    indexer = self._filter_indexer_tolerance(target_values, indexer, tolerance)
[32mPASSED![0m
Time :  1224.47 seconds

[36m[[[pandas-40180]]][0m
[31mFAILED...[0m
Time :  1.07 seconds

[36m[[[pandas-41155]]][0m
Timeout!
Time :  3600.08 seconds

[36m[[[pandas-41406]]][0m
[[[ Node ]]]
try:
    res = idx._get_string_slice(key)
    warnings.warn('Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.', FutureWarning, stacklevel=3)
    return res
except (KeyError, ValueError, NotImplementedError):
    return None
except TypeError:
    return None
[32mPASSED![0m
Time :  6.29 seconds

[36m[[[pandas-41915]]][0m
/home/wonseok/benchmark/pandas-41915/pandas/tests/indexes/multi/test_setops.py not exists
/home/wonseok/benchmark/pandas-41915/pandas/tests/indexes/multi/test_setops.py not exists
[31mFAILED...[0m
Time :  9.0 seconds

PASSED :  24 / 45
[36m[[[rasa-8704]]][0m
[[[ Node ]]]
def __init__(self, message: Text) -> None:
    self.message = message
    super(UnsupportedModelError, self).__init__(message)
[32mPASSED![0m
Time :  8.27 seconds

PASSED :  1 / 1
[36m[[[requests-3179]]][0m
[[[ Node ]]]
if self.content and (not self.encoding and len(self.content) > 3):
    encoding = guess_json_utf(self.content)
    if encoding is not None:
        try:
            return complexjson.loads(self.content.decode(encoding), **kwargs)
        except UnicodeDecodeError:
            pass
[32mPASSED![0m
Time :  33.22 seconds

[36m[[[requests-3368]]][0m
[[[ Node ]]]
if self._content_consumed and isinstance(self._content, bool):
    raise StreamConsumedError()
elif chunk_size and (not isinstance(chunk_size, int)):
    raise TypeError('chunk_size must be an int, it is instead a %s.' % type(chunk_size))
[32mPASSED![0m
Time :  24.41 seconds

[36m[[[requests-3390]]][0m
[[[ Node ]]]
def check_header_validity(header):
    """Verifies that header value doesn't contain leading whitespace or
    return characters. This prevents unintended header injection.

    :param header: tuple, in the format (name, value).
    """
    (name, value) = header
    if isinstance(value, bytes):
        pat = _CLEAN_HEADER_REGEX_BYTE
    else:
        pat = _CLEAN_HEADER_REGEX_STR
    if not (isinstance(value, str) or isinstance(value, bytes)):
        raise InvalidHeader
    if not pat.match(value):
        raise InvalidHeader('Invalid return character or leading space in header: %s' % name)
[32mPASSED![0m
Time :  625.69 seconds

[36m[[[requests-4723]]][0m
[[[ Node ]]]
def should_bypass_proxies(url, no_proxy):
    """
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = urlparse(url)
    if isinstance(parsed.hostname, type(None)):
        return True
    if no_proxy:
        no_proxy = (host for host in no_proxy.replace(' ', '').split(',') if host)
        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{0}'.format(parsed.port)
            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    return True
    with set_environ('no_proxy', no_proxy_arg):
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False
    if bypass:
        return True
    return False
[32mPASSED![0m
Time :  15.97 seconds

PASSED :  4 / 4
[36m[[[rich-919]]][0m
[33mERROR...[0m
  File "/home/wonseok/pyfix/my_tool/test_main.py", line 317, in run
    works.work()
  File "/home/wonseok/pyfix/my_tool/work.py", line 1356, in work
    self.patch_only_once(synthe, ranking_localize)
  File "/home/wonseok/pyfix/my_tool/work.py", line 1134, in patch_only_once
    self.spec_inference(synthe, ranking_localize)
  File "/home/wonseok/pyfix/my_tool/work.py", line 660, in spec_inference
    self.func_spec_inference(synthe)
  File "/home/wonseok/pyfix/my_tool/work.py", line 548, in func_spec_inference
    synthe.synthesize(node, neg_filename, neg_funcname, neg_classname, neg_info['args'], self.pos_func_infos, None, context_aware, None, None, test, self.total_test_num, func_patch=True)
  File "/home/wonseok/pyfix/my_tool/synthesizer/synthesize.py", line 98, in synthesize
    temp_synthesize.template_synthesize(node)
  File "/home/wonseok/pyfix/my_tool/synthesizer/template_synthesizer.py", line 559, in template_synthesize
    self.validation(final_node, targets)
  File "/home/wonseok/pyfix/my_tool/synthesizer/template_synthesizer.py", line 450, in validation
    self.validator.validate(node, self.filename, target, self.test, self.total_test_num)
  File "/home/wonseok/pyfix/my_tool/synthesizer/validator.py", line 186, in validate
    shutil.move(filename + "_origin", filename)
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/shutil.py", line 820, in move
    copy_function(src, real_dst)
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/shutil.py", line 435, in copy2
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/shutil.py", line 264, in copyfile
    with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:
[Errno 2] No such file or directory: '/home/wonseok/benchmark/rich-919/rich/console.py_origin'
Time :  41.42 seconds

PASSED :  0 / 1
[36m[[[salt-33908]]][0m
[[[ Node ]]]
for key in set(new or {}).union(old or {}):
    if isinstance(new, type(None)):
        new = {}
    if key not in old:
        ret[key] = {'old': '', 'new': new[key]}
    elif key not in new:
        ret[key] = {'new': '', 'old': old[key]}
    elif new[key] != old[key]:
        ret[key] = {'old': old[key], 'new': new[key]}
[32mPASSED![0m
Time :  7.91 seconds

[36m[[[salt-38947]]][0m
[[[ Node ]]]
return ' '.join(['-o {0}'.format(opt) for opt in ('' if self.ssh_options is None else self.ssh_options)])
[32mPASSED![0m
Time :  3.45 seconds

[36m[[[salt-50958]]][0m
[31mFAILED...[0m
Time :  0.02 seconds

[36m[[[salt-52624]]][0m
[[[ Node ]]]
if (isinstance(self.opts['batch'], str)) and '%' in self.opts['batch']:
    res = partition(float(self.opts['batch'].strip('%')))
    if res < 1:
        return int(math.ceil(res))
    else:
        return int(res)
else:
    return int(self.opts['batch'])
[32mPASSED![0m
Time :  5.0 seconds

[36m[[[salt-52710]]][0m
[31mFAILED...[0m
Time :  0.01 seconds

[36m[[[salt-53394]]][0m
[[[ Node ]]]
def __decompressContent(coding, pgctnt):
    """
    Decompress returned HTTP content depending on the specified encoding.
    Currently supports identity/none, deflate, and gzip, which should
    cover 99%+ of the content on the internet.
    """
    if isinstance(pgctnt, type(None)):
        return pgctnt
    log.trace('Decompressing %s byte content with compression type: %s', len(pgctnt), coding)
    if coding == 'deflate':
        pgctnt = zlib.decompress(pgctnt, -zlib.MAX_WBITS)
    elif coding == 'gzip':
        buf = io.BytesIO(pgctnt)
        f = gzip.GzipFile(fileobj=buf)
        pgctnt = f.read()
    elif coding == 'sdch':
        raise ValueError('SDCH compression is not currently supported')
    elif coding == 'br':
        raise ValueError('Brotli compression is not currently supported')
    elif coding == 'compress':
        raise ValueError('LZW compression is not currently supported')
    elif coding == 'identity':
        pass
    log.trace('Content size after decompression: %s', len(pgctnt))
    return pgctnt
[32mPASSED![0m
Time :  9.6 seconds

[36m[[[salt-54240]]][0m
[31mFAILED...[0m
Time :  134.79 seconds

[36m[[[salt-54785]]][0m
[31mFAILED...[0m
Time :  560.71 seconds

[36m[[[salt-56094]]][0m
[[[ Node ]]]
def find_module(self, module_name, package_path=None):
    if module_name.startswith('tornado'):
        return self
    return None
[32mPASSED![0m
Time :  2.61 seconds

[36m[[[salt-56381]]][0m
[[[ Node ]]]
ret['comment'] = '  '.join(['' if not ret['comment'] else str(ret['comment']), 'The state would be retried every {1} seconds (with a splay of up to {3} seconds) a maximum of {0} times or until a result of {2} is returned'.format(low['retry']['attempts'], low['retry']['interval'], low['retry']['until'], low['retry']['splay'])])
[32mPASSED![0m
Time :  118.87 seconds

PASSED :  6 / 10
[36m[[[sanic-1334]]][0m
[[[ Node ]]]
for bp in chain(blueprints):
    if isinstance(bp.url_prefix, type(None)):
        bp.url_prefix = ''
    bp.url_prefix = url_prefix + bp.url_prefix
    bps.append(bp)
[32mPASSED![0m
Time :  1.97 seconds

[36m[[[sanic-2008-1]]][0m
[[[ Node ]]]
def register(app, uri: str, file_or_directory: Union[str, bytes, PurePath], pattern, use_modified_since, use_content_range, stream_large_files, name: str='static', host=None, strict_slashes=None, content_type=None):
    """
    Register a static directory handler with Sanic by adding a route to the
    router and registering a handler.

    :param app: Sanic
    :param file_or_directory: File or directory path to serve from
    :type file_or_directory: Union[str,bytes,Path]
    :param uri: URL to serve from
    :type uri: str
    :param pattern: regular expression used to match files in the URL
    :param use_modified_since: If true, send file modified time, and return
                               not modified if the browser's matches the
                               server's
    :param use_content_range: If true, process header for range requests
                              and sends the file part that is requested
    :param stream_large_files: If true, use the file_stream() handler rather
                              than the file() handler to send the file
                              If this is an integer, this represents the
                              threshold size to switch to file_stream()
    :param name: user defined name used for url_for
    :type name: str
    :param content_type: user defined content type for header
    :return: registered static routes
    :rtype: List[sanic.router.Route]
    """
    if isinstance(file_or_directory, bytes):
        file_or_directory = file_or_directory.decode('utf-8')
    elif isinstance(file_or_directory, PurePath):
        file_or_directory = str(file_or_directory)
    if not (isinstance(file_or_directory, str)):
        raise ValueError
    if not path.isfile(file_or_directory):
        uri += '<file_uri:' + pattern + '>'
    if not name.startswith('_static_'):
        name = f'_static_{name}'
    _handler = wraps(_static_request_handler)(partial(_static_request_handler, file_or_directory, use_modified_since, use_content_range, stream_large_files, content_type=content_type))
    (_routes, _) = app.route(uri, methods=['GET', 'HEAD'], name=name, host=host, strict_slashes=strict_slashes)(_handler)
    return _routes
[32mPASSED![0m
Time :  75.47 seconds

[36m[[[sanic-2008-2]]][0m
[[[ Node ]]]
async def _static_request_handler(file_or_directory, use_modified_since, use_content_range, stream_large_files, request, content_type=None, file_uri=None):
    import pathlib
    if isinstance(file_or_directory, pathlib.PosixPath):
        file_or_directory = str(file_or_directory)
    elif isinstance(file_or_directory, bytes):
        file_or_directory = str(file_or_directory, 'utf-8')
    if file_uri and '../' in file_uri:
        raise InvalidUsage('Invalid URL')
    root_path = file_path = file_or_directory
    if file_uri:
        file_path = path.join(file_or_directory, sub('^[/]*', '', file_uri))
    file_path = path.abspath(unquote(file_path))
    if not file_path.startswith(path.abspath(unquote(root_path))):
        error_logger.exception(f'File not found: path={file_or_directory}, relative_url={file_uri}')
        raise FileNotFound('File not found', path=file_or_directory, relative_url=file_uri)
    try:
        headers = {}
        stats = None
        if use_modified_since:
            stats = await stat_async(file_path)
            modified_since = strftime('%a, %d %b %Y %H:%M:%S GMT', gmtime(stats.st_mtime))
            if request.headers.get('If-Modified-Since') == modified_since:
                return HTTPResponse(status=304)
            headers['Last-Modified'] = modified_since
        _range = None
        if use_content_range:
            _range = None
            if not stats:
                stats = await stat_async(file_path)
            headers['Accept-Ranges'] = 'bytes'
            headers['Content-Length'] = str(stats.st_size)
            if request.method != 'HEAD':
                try:
                    _range = ContentRangeHandler(request, stats)
                except HeaderNotFound:
                    pass
                else:
                    del headers['Content-Length']
                    for (key, value) in _range.headers.items():
                        headers[key] = value
        headers['Content-Type'] = content_type or guess_type(file_path)[0] or 'text/plain'
        if request.method == 'HEAD':
            return HTTPResponse(headers=headers)
        else:
            if stream_large_files:
                if type(stream_large_files) == int:
                    threshold = stream_large_files
                else:
                    threshold = 1024 * 1024
                if not stats:
                    stats = await stat_async(file_path)
                if stats.st_size >= threshold:
                    return await file_stream(file_path, headers=headers, _range=_range)
            return await file(file_path, headers=headers, _range=_range)
    except ContentRangeError:
        raise
    except Exception:
        error_logger.exception(f'File not found: path={file_or_directory}, relative_url={file_uri}')
        raise FileNotFound('File not found', path=file_or_directory, relative_url=file_uri)
[32mPASSED![0m
Time :  52.59 seconds

PASSED :  3 / 3
[36m[[[scikitlearn-7064]]][0m
[[[ Node ]]]
def fit(self, X, y, sample_weight=None):
    """Fit the SVM model according to the given training data.

        Parameters
        ----------
        X : {array-like, sparse matrix}, shape (n_samples, n_features)
            Training vectors, where n_samples is the number of samples
            and n_features is the number of features.
            For kernel="precomputed", the expected shape of X is
            (n_samples, n_samples).

        y : array-like, shape (n_samples,)
            Target values (class labels in classification, real numbers in
            regression)

        sample_weight : array-like, shape (n_samples,)
            Per-sample weights. Rescale C per sample. Higher weights
            force the classifier to put more emphasis on these points.

        Returns
        -------
        self : object
            Returns self.

        Notes
        ------
        If X and y are not C-ordered and contiguous arrays of np.float64 and
        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.

        If X is a dense array, then the other methods will not support sparse
        matrices as input.
        """
    rnd = check_random_state(self.random_state)
    sparse = sp.isspmatrix(X)
    if sparse and self.kernel == 'precomputed':
        raise TypeError('Sparse precomputed kernels are not supported.')
    self._sparse = sparse and (not callable(self.kernel))
    (X, y) = check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr')
    y = self._validate_targets(y)
    sample_weight = np.asarray([] if sample_weight is None else sample_weight, dtype=np.float64)
    solver_type = LIBSVM_IMPL.index(self._impl)
    if solver_type != 2 and X.shape[0] != y.shape[0]:
        raise ValueError('X and y have incompatible shapes.\n' + 'X has %s samples, but y has %s.' % (X.shape[0], y.shape[0]))
    if self.kernel == 'precomputed' and X.shape[0] != X.shape[1]:
        raise ValueError('X.shape[0] should be equal to X.shape[1]')
    if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:
        raise ValueError('sample_weight and X have incompatible shapes: %r vs %r\nNote: Sparse matrices cannot be indexed w/boolean masks (use `indices=True` in CV).' % (sample_weight.shape, X.shape))
    if self.gamma == 'auto':
        self._gamma = 1.0 / X.shape[1]
    else:
        self._gamma = self.gamma
    kernel = self.kernel
    if callable(kernel):
        kernel = 'precomputed'
    fit = self._sparse_fit if self._sparse else self._dense_fit
    if self.verbose:
        print('[LibSVM]', end='')
    seed = rnd.randint(np.iinfo('i').max)
    if isinstance(kernel, bytes):
        kernel = str(kernel, 'utf-8')
    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)
    self.shape_fit_ = X.shape
    self._intercept_ = self.intercept_.copy()
    self._dual_coef_ = self.dual_coef_
    if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:
        self.intercept_ *= -1
        self.dual_coef_ = -self.dual_coef_
    return self
[32mPASSED![0m
Time :  3.91 seconds

[36m[[[scikitlearn-7259]]][0m
[31mFAILED...[0m
Time :  245.27 seconds

[36m[[[scikitlearn-8973]]][0m
[[[ Node ]]]
folds = list(cv.split(X, y=y))
[32mPASSED![0m
Time :  3.12 seconds

[36m[[[scikitlearn-12603]]][0m
[[[ Node ]]]
def assert_raise_message(exceptions, message, function, *args, **kwargs):
    """Helper function to test the message raised in an exception.

    Given an exception, a callable to raise the exception, and
    a message string, tests that the correct exception is raised and
    that the message is a substring of the error thrown. Used to test
    that the specific message thrown during an exception is correct.

    Parameters
    ----------
    exceptions : exception or tuple of exception
        An Exception object.

    message : str
        The error message or a substring of the error message.

    function : callable
        Callable object to raise error.

    *args : the positional arguments to `function`.

    **kwargs : the keyword arguments to `function`.
    """
    import sklearn
    if not isinstance(function, sklearn.gaussian_process.kernels.RationalQuadratic):
        try:
            function(*args, **kwargs)
        except exceptions as e:
            error_message = str(e)
            if message not in error_message:
                raise AssertionError('Error message does not include the expected string: %r. Observed error message: %r' % (message, error_message))
        else:
            if isinstance(exceptions, tuple):
                names = ' or '.join((e.__name__ for e in exceptions))
            else:
                names = exceptions.__name__
            raise AssertionError('%s not raised by %s' % (names, function.__name__))
[32mPASSED![0m
Time :  307.75 seconds

[36m[[[scikitlearn-17233]]][0m
[31mFAILED...[0m
Time :  1483.4 seconds

PASSED :  3 / 5
[36m[[[tornado-1689]]][0m
[[[ Node ]]]
def check_xsrf_cookie(self):
    """Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.

        To prevent cross-site request forgery, we set an ``_xsrf``
        cookie and include the same value as a non-cookie
        field with all ``POST`` requests. If the two do not match, we
        reject the form submission as a potential forgery.

        The ``_xsrf`` value may be set as either a form field named ``_xsrf``
        or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``
        (the latter is accepted for compatibility with Django).

        See http://en.wikipedia.org/wiki/Cross-site_request_forgery

        Prior to release 1.1.1, this check was ignored if the HTTP header
        ``X-Requested-With: XMLHTTPRequest`` was present.  This exception
        has been shown to be insecure and has been removed.  For more
        information please see
        http://www.djangoproject.com/weblog/2011/feb/08/security/
        http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails

        .. versionchanged:: 3.2.2
           Added support for cookie version 2.  Both versions 1 and 2 are
           supported.
        """
    token = self.get_argument('_xsrf', None) or self.request.headers.get('X-Xsrftoken') or self.request.headers.get('X-Csrftoken')
    if not token:
        raise HTTPError(403, "'_xsrf' argument missing from POST")
    (_, token, _) = self._decode_xsrf_token(token)
    (_, expected_token, _) = self._get_raw_xsrf_token()
    if isinstance(token, type(None)):
        raise HTTPError(403, ".*'_xsrf' argument has invalid format")
    if not _time_independent_equals(utf8(token), utf8(expected_token)):
        raise HTTPError(403, 'XSRF cookie does not match POST argument')
[32mPASSED![0m
Time :  198.99 seconds

PASSED :  1 / 1
[36m[[[transformers-8052]]][0m
[31mFAILED...[0m
Time :  0.02 seconds

PASSED :  0 / 1
[36m[[[Zappa-388]]][0m
[[[ Node ]]]
environ['CONTENT_LENGTH'] = str(len(unicode() if body is None else body))
[32mPASSED![0m
Time :  151.95 seconds

[36m[[[Zappa-1434]]][0m
[33mERROR...[0m
  File "/home/wonseok/pyfix/my_tool/test_main.py", line 317, in run
    works.work()
  File "/home/wonseok/pyfix/my_tool/work.py", line 1356, in work
    self.patch_only_once(synthe, ranking_localize)
  File "/home/wonseok/pyfix/my_tool/work.py", line 1134, in patch_only_once
    self.spec_inference(synthe, ranking_localize)
  File "/home/wonseok/pyfix/my_tool/work.py", line 723, in spec_inference
    context_aware = ContextAware(typecheck_candidates, [neg_file_node])
  File "/home/wonseok/pyfix/my_tool/context/context.py", line 16, in __init__
    for child in ast.iter_child_nodes(root)),
  File "/home/wonseok/pyfix/my_tool/context/context.py", line 13, in depth_ast
    #print(table)
  File "/home/wonseok/pyfix/my_tool/context/context.py", line 13, in <genexpr>
    #print(table)
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/ast.py", line 261, in iter_child_nodes
    for name, field in iter_fields(node):
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/ast.py", line 249, in iter_fields
    for field in node._fields:
'list' object has no attribute '_fields'
Time :  126.71 seconds

PASSED :  1 / 2
Total :  57 / 95
