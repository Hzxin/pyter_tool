nohup: ignoring input
[36m[[[airflow-5686]]][0m
[[[ Node ]]]
if self.http_conn_id:
    conn = self.get_connection(self.http_conn_id)
    if conn.host and '://' in conn.host:
        self.base_url = conn.host
    else:
        schema = conn.schema if conn.schema else 'http'
        self.base_url = schema + '://' + ('' if isinstance(conn.host, type(None)) else conn.host)
    if conn.port:
        self.base_url = self.base_url + ':' + str(conn.port)
    if conn.login:
        session.auth = (conn.login, conn.password)
    if conn.extra:
        try:
            session.headers.update(conn.extra_dejson)
        except TypeError:
            self.log.warn('Connection to %s has invalid extra field.', conn.host)
[32mPASSED![0m
Time :  11.92 seconds

[36m[[[airflow-14513]]][0m
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
None Type Casting Other Type
Type :  None
[[[ Node ]]]
while True:
    logs = self.read_pod_logs(pod, timestamps=True, since_seconds=read_logs_since_sec)
    for line in logs:
        (timestamp, message) = self.parse_log_line(line.decode('utf-8'))
        last_log_time = pendulum.parse(timestamp)
        self.log.info(message)
    time.sleep(1)
    if not self.base_container_is_running(pod):
        break
    self.log.warning('Pod %s log read interrupted', pod.metadata.name)
    if isinstance(last_log_time, type(None)):
        continue
    delta = pendulum.now() - last_log_time
    read_logs_since_sec = math.ceil(delta.total_seconds())
[32mPASSED![0m
Time :  154.68 seconds

PASSED :  2 / 12
PASSED :  0 / 1
PASSED :  0 / 1
[36m[[[core-1972]]][0m
[[[ Node ]]]
check = condition.from_config(action)(self.hass, variables)
[32mPASSED![0m
Time :  2.32 seconds

[36m[[[core-8065]]][0m
[[[ Node ]]]
ids = sorted(self.entities, key=lambda x: x if isinstance(self.entities[x].name, type(None)) else self.entities[x].name)
[32mPASSED![0m
Time :  14.32 seconds

[36m[[[core-20233]]][0m
[[[ Node ]]]
try:
    variables['value_json'] = json.loads(value)
except ValueError:
    pass
except TypeError:
    pass
[32mPASSED![0m
Time :  3.82 seconds

[36m[[[core-32222]]][0m
None Type Casting Other Type
Type :  float
None Type Casting Other Type
Type :  float
[[[ Node ]]]
since_last_seen = dt_util.utcnow() - dt_util.utc_from_timestamp(float(0 if isinstance(self.client.last_seen, type(None)) else self.client.last_seen))
[32mPASSED![0m
Time :  24.53 seconds

[36m[[[core-40034]]][0m
[[[ Node ]]]
while to_process:
    config = to_process.popleft()
    import homeassistant
    if isinstance(config, homeassistant.helpers.template.Template):
        continue
    condition = config[CONF_CONDITION]
    if condition in ('and', 'not', 'or'):
        to_process.extend(config['conditions'])
        continue
    entity_ids = config.get(CONF_ENTITY_ID)
    if isinstance(entity_ids, str):
        entity_ids = [entity_ids]
    if entity_ids is not None:
        referenced.update(entity_ids)
[[[ Node ]]]
while to_process:
    config = to_process.popleft()
    import homeassistant
    if isinstance(config, homeassistant.helpers.template.Template):
        continue
    condition = config[CONF_CONDITION]
    if condition in ('and', 'not', 'or'):
        to_process.extend(config['conditions'])
        continue
    if condition != 'device':
        continue
    device_id = config.get(CONF_DEVICE_ID)
    if device_id is not None:
        referenced.add(device_id)
[32mPASSED![0m
Time :  16.89 seconds

PASSED :  5 / 16
PASSED :  0 / 3
PASSED :  0 / 4
[36m[[[pandas-15941]]][0m
[[[ Node ]]]
def is_string_dtype(arr_or_dtype):
    if isinstance(arr_or_dtype, type(None)):
        return False
    dtype = _get_dtype(arr_or_dtype)
    return dtype.kind in ('O', 'S', 'U') and (not is_period_dtype(dtype))
[[[ Node ]]]
def is_timedelta64_ns_dtype(arr_or_dtype):
    if isinstance(arr_or_dtype, type(None)):
        return False
    tipo = _get_dtype(arr_or_dtype)
    return tipo == _TD_DTYPE
[[[ Node ]]]
def is_string_like_dtype(arr_or_dtype):
    if isinstance(arr_or_dtype, type(None)):
        return False
    dtype = _get_dtype(arr_or_dtype)
    return dtype.kind in ('S', 'U')
[32mPASSED![0m
Time :  11.47 seconds

[36m[[[pandas-17609]]][0m
[[[ Node ]]]
defaults = ('',) * n_wo_defaults + tuple(spec.defaults)
[32mPASSED![0m
Time :  4.26 seconds

[36m[[[pandas-19276]]][0m
[[[ Node ]]]
def _assert_tzawareness_compat(self, other):
    import pandas
    if isinstance(other, pandas._libs.tslibs.nattype.NaTType):
        return None
    other_tz = getattr(other, 'tzinfo', None)
    if is_datetime64tz_dtype(other):
        other_tz = other.dtype.tz
    if self.tz is None:
        if other_tz is not None:
            raise TypeError('Cannot compare tz-naive and tz-aware datetime-like objects.')
    elif other_tz is None:
        raise TypeError('Cannot compare tz-naive and tz-aware datetime-like objects')
[32mPASSED![0m
Time :  10.51 seconds

[36m[[[pandas-20968]]][0m
[[[ Node ]]]
def boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharey=True, sharex=True, **kwds):
    """
    Make box plots from DataFrameGroupBy data.

    Parameters
    ----------
    grouped : Grouped DataFrame
    subplots :
        * ``False`` - no subplots will be used
        * ``True`` - create a subplot for each group
    column : column name or list of names, or vector
        Can be any valid input to groupby
    fontsize : int or string
    rot : label rotation angle
    grid : Setting this to True will show the grid
    ax : Matplotlib axis object, default None
    figsize : A tuple (width, height) in inches
    layout : tuple (optional)
        (rows, columns) for the layout of the plot
    `**kwds` : Keyword Arguments
        All other plotting keyword arguments to be passed to
        matplotlib's boxplot function

    Returns
    -------
    dict of key/value = group key/DataFrame.boxplot return value
    or DataFrame.boxplot return value in case subplots=figures=False

    Examples
    --------
    >>> import pandas
    >>> import numpy as np
    >>> import itertools
    >>>
    >>> tuples = [t for t in itertools.product(range(1000), range(4))]
    >>> index = pandas.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])
    >>> data = np.random.randn(len(index),4)
    >>> df = pandas.DataFrame(data, columns=list('ABCD'), index=index)
    >>>
    >>> grouped = df.groupby(level='lvl1')
    >>> boxplot_frame_groupby(grouped)
    >>>
    >>> grouped = df.unstack(level='lvl1').groupby(level=0, axis=1)
    >>> boxplot_frame_groupby(grouped, subplots=False)
    """
    _raise_if_no_mpl()
    _converter._WARN = False
    if subplots is True:
        naxes = len(grouped)
        (fig, axes) = _subplots(naxes=naxes, squeeze=False, ax=ax, sharex=sharex, sharey=sharey, figsize=figsize, layout=layout)
        axes = _flatten(axes)
        from pandas.core.series import Series
        ret = Series()
        for ((key, group), ax) in zip(grouped, axes):
            d = group.boxplot(ax=ax, column=column, fontsize=fontsize, rot=rot, grid=grid, **kwds)
            ax.set_title(pprint_thing(key))
            ret.loc[key] = d
        fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9, wspace=0.2)
    else:
        from pandas.core.reshape.concat import concat
        (keys, frames) = zip(*grouped)
        if grouped.axis == 0:
            df = concat(frames, keys=keys, axis=1)
        elif len(frames) > 1:
            df = frames[0].join(frames[1:])
        else:
            df = frames[0]
        ret = df.boxplot(column=column, fontsize=fontsize, rot=rot, grid=grid, ax=ax, figsize=figsize, layout=layout, **kwds)
    return ret
[32mPASSED![0m
Time :  56.15 seconds

[36m[[[pandas-21540]]][0m
[[[ Node ]]]
result.rename(columns=lambda x: record_prefix + str(x), inplace=True)
[32mPASSED![0m
Time :  28.61 seconds

[36m[[[pandas-21590]]][0m
[[[ Node ]]]
if is_numeric_dtype(values) or is_timedelta64_dtype(values):
    arr = operator.neg(values)
else:
    import numpy
    if isinstance(values, numpy.ndarray) and values.dtype.type is numpy.object_:
        arr = operator.neg(values)
    else:
        raise TypeError('Unary negative expects numeric dtype, not {}'.format(values.dtype))
[[[ Node ]]]
if is_numeric_dtype(values) or is_timedelta64_dtype(values):
    arr = operator.pos(values)
else:
    import numpy
    if isinstance(values, numpy.ndarray) and values.dtype.type is numpy.object_:
        arr = operator.pos(values)
    else:
        raise TypeError('Unary plus expects numeric dtype, not {}'.format(values.dtype))
[32mPASSED![0m
Time :  402.53 seconds

[36m[[[pandas-22072]]][0m
[[[ Node ]]]
cat = Categorical(values, ordered=False)
[32mPASSED![0m
Time :  6.83 seconds

[36m[[[pandas-22198]]][0m
[[[ Node ]]]
def asof(self, where, subset=None):
    import pandas
    if isinstance(where, pandas._libs.tslibs.timestamps.Timestamp):
        return np.nan
    '\n        The last row without any NaN is taken (or the last row without\n        NaN considering only the subset of columns in the case of a DataFrame)\n\n        .. versionadded:: 0.19.0 For DataFrame\n\n        If there is no good value, NaN is returned for a Series\n        a Series of NaN values for a DataFrame\n\n        Parameters\n        ----------\n        where : date or array of dates\n        subset : string or list of strings, default None\n           if not None use these columns for NaN propagation\n\n        Notes\n        -----\n        Dates are assumed to be sorted\n        Raises if this is not the case\n\n        Returns\n        -------\n        where is scalar\n\n          - value or NaN if input is Series\n          - Series if input is DataFrame\n\n        where is Index: same shape object as input\n\n        See Also\n        --------\n        merge_asof\n\n        '
    if isinstance(where, compat.string_types):
        from pandas import to_datetime
        where = to_datetime(where)
    if not self.index.is_monotonic:
        raise ValueError('asof requires a sorted index')
    is_series = isinstance(self, ABCSeries)
    if is_series:
        if subset is not None:
            raise ValueError('subset is not valid for Series')
    elif self.ndim > 2:
        raise NotImplementedError('asof is not implemented for {type}'.format(type=type(self)))
    else:
        if subset is None:
            subset = self.columns
        if not is_list_like(subset):
            subset = [subset]
    is_list = is_list_like(where)
    if not is_list:
        start = self.index[0]
        if isinstance(self.index, PeriodIndex):
            where = Period(where, freq=self.index.freq).ordinal
            start = start.ordinal
        if where < start:
            if not is_series:
                from pandas import Series
                return Series(index=self.columns, name=where)
            return np.nan
        if is_series:
            loc = self.index.searchsorted(where, side='right')
            if loc > 0:
                loc -= 1
            values = self._values
            while loc > 0 and isna(values[loc]):
                loc -= 1
            return values[loc]
    if not isinstance(where, Index):
        where = Index(where) if is_list else Index([where])
    nulls = self.isna() if is_series else self[subset].isna().any(1)
    if nulls.all():
        if is_series:
            return self._constructor(np.nan, index=where, name=self.name)
        elif is_list:
            from pandas import DataFrame
            return DataFrame(np.nan, index=where, columns=self.columns)
        else:
            from pandas import Series
            return Series(np.nan, index=self.columns, name=where[0])
    locs = self.index.asof_locs(where, ~nulls.values)
    missing = locs == -1
    data = self.take(locs, is_copy=False)
    data.index = where
    data.loc[missing] = np.nan
    return data if is_list else data.iloc[-1]
[32mPASSED![0m
Time :  8.95 seconds

[36m[[[pandas-22804]]][0m
[[[ Node ]]]
def _pull_field(js, spec):
    result = js
    if isinstance(result, str):
        return result
    if isinstance(spec, list):
        for field in spec:
            result = result[field]
    else:
        result = result[spec]
    return result
[32mPASSED![0m
Time :  4.18 seconds

[36m[[[pandas-25533]]][0m
[[[ Node ]]]
try:
    if takeable:
        self._values[label] = value
    else:
        self.index._engine.set_value(self._values, label, value)
except KeyError:
    self.loc[label] = value
except TypeError:
    self.loc[label] = value
[32mPASSED![0m
Time :  5.15 seconds

[36m[[[pandas-25759]]][0m
Timeout!
Time :  3600.07 seconds

[36m[[[pandas-26765]]][0m
[[[ Node ]]]
try:
    loc = cat.categories.get_loc(key)
except KeyError:
    return False
except TypeError:
    return False
[32mPASSED![0m
Time :  21.04 seconds

[36m[[[pandas-32953]]][0m
[[[ Node ]]]
if isinstance(objs, dict) or isinstance(objs, abc.Mapping):
    if keys is None:
        keys = list(objs.keys())
    objs = [objs[k] for k in keys]
else:
    objs = list(objs)
[32mPASSED![0m
Time :  5.89 seconds

[36m[[[pandas-33373]]][0m
[[[ Node ]]]
try:
    new_data = to_datetime(new_data, errors='raise', unit=date_unit)
except (ValueError, OverflowError):
    continue
except TypeError:
    continue
[32mPASSED![0m
Time :  15.98 seconds

[36m[[[pandas-36950]]][0m
[[[ Node ]]]
(result, how) = self._aggregate(func, axis, *args, **kwargs)
[32mPASSED![0m
Time :  5.25 seconds

[36m[[[pandas-38431]]][0m
[[[ Node ]]]
j = i if isinstance(self.index_col, type(None)) else self.index_col[i]
[32mPASSED![0m
Time :  17.85 seconds

PASSED :  15 / 69
[36m[[[rasa-8704]]][0m
[31mFAILED...[0m
Time :  0.0 seconds

PASSED :  0 / 3
[36m[[[requests-3179]]][0m
[[[ Node ]]]
if self.content and (not self.encoding and len(self.content) > 3):
    encoding = guess_json_utf(self.content)
    if encoding is not None:
        try:
            return complexjson.loads(self.content.decode(encoding), **kwargs)
        except UnicodeDecodeError:
            pass
[32mPASSED![0m
Time :  41.75 seconds

[36m[[[requests-3368]]][0m
[[[ Node ]]]
if self._content_consumed and isinstance(self._content, bool):
    raise StreamConsumedError()
elif chunk_size and (not isinstance(chunk_size, int)):
    raise TypeError('chunk_size must be an int, it is instead a %s.' % type(chunk_size))
[32mPASSED![0m
Time :  26.12 seconds

[36m[[[requests-3390]]][0m
None Type Casting Other Type
Type :  requests.models.Request
None Type Casting Other Type
Type :  requests.models.Request
None Type Casting Other Type
Type :  requests.models.Request
None Type Casting Other Type
Type :  requests.models.Request
None Type Casting Other Type
Type :  requests.models.Request
None Type Casting Other Type
Type :  requests.models.Request
[31mFAILED...[0m
Time :  605.14 seconds

[36m[[[requests-4723]]][0m
[[[ Node ]]]
def should_bypass_proxies(url, no_proxy):
    """
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """
    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy('no_proxy')
    parsed = urlparse(url)
    if parsed.hostname and no_proxy:
        no_proxy = (host for host in no_proxy.replace(' ', '').split(',') if host)
        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += ':{0}'.format(parsed.port)
            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    return True
    with set_environ('no_proxy', no_proxy_arg):
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False
    if bypass:
        return True
    return False
[32mPASSED![0m
Time :  10.81 seconds

PASSED :  3 / 8
PASSED :  0 / 1
[36m[[[salt-33908]]][0m
[[[ Node ]]]
for key in set(new or {}).union(old or {}):
    if isinstance(key, unicode):
        return ret
    if key not in old:
        ret[key] = {'old': '', 'new': new[key]}
    elif key not in new:
        ret[key] = {'new': '', 'old': old[key]}
    elif new[key] != old[key]:
        ret[key] = {'old': old[key], 'new': new[key]}
[32mPASSED![0m
Time :  20.96 seconds

[36m[[[salt-38947]]][0m
[31mFAILED...[0m
Time :  21.54 seconds

[36m[[[salt-52624]]][0m
[[[ Node ]]]
try:
    if '%' in self.opts['batch']:
        res = partition(float(self.opts['batch'].strip('%')))
        if res < 1:
            return int(math.ceil(res))
        else:
            return int(res)
    else:
        return int(self.opts['batch'])
except ValueError:
    if not self.quiet:
        salt.utils.stringutils.print_cli('Invalid batch data sent: {0}\nData must be in the form of %10, 10% or 3'.format(self.opts['batch']))
except TypeError:
    if not self.quiet:
        salt.utils.stringutils.print_cli('Invalid batch data sent: {0}\nData must be in the form of %10, 10% or 3'.format(self.opts['batch']))
[32mPASSED![0m
Time :  3.52 seconds

[36m[[[salt-53394]]][0m
[[[ Node ]]]
def __decompressContent(coding, pgctnt):
    if isinstance(pgctnt, type(None)):
        return pgctnt
    '\n    Decompress returned HTTP content depending on the specified encoding.\n    Currently supports identity/none, deflate, and gzip, which should\n    cover 99%+ of the content on the internet.\n    '
    log.trace('Decompressing %s byte content with compression type: %s', len(pgctnt), coding)
    if coding == 'deflate':
        pgctnt = zlib.decompress(pgctnt, -zlib.MAX_WBITS)
    elif coding == 'gzip':
        buf = io.BytesIO(pgctnt)
        f = gzip.GzipFile(fileobj=buf)
        pgctnt = f.read()
    elif coding == 'sdch':
        raise ValueError('SDCH compression is not currently supported')
    elif coding == 'br':
        raise ValueError('Brotli compression is not currently supported')
    elif coding == 'compress':
        raise ValueError('LZW compression is not currently supported')
    elif coding == 'identity':
        pass
    log.trace('Content size after decompression: %s', len(pgctnt))
    return pgctnt
[32mPASSED![0m
Time :  23.87 seconds

[36m[[[salt-56094]]][0m
[[[ Node ]]]
def find_module(self, module_name, package_path=None):
    if module_name.startswith('tornado'):
        return self
    return None
[32mPASSED![0m
Time :  2.21 seconds

[36m[[[salt-56381]]][0m
[[[ Node ]]]
ret['comment'] = '  '.join(['' if not ret['comment'] else str(ret['comment']), 'The state would be retried every {1} seconds (with a splay of up to {3} seconds) a maximum of {0} times or until a result of {2} is returned'.format(low['retry']['attempts'], low['retry']['interval'], low['retry']['until'], low['retry']['splay'])])
[32mPASSED![0m
Time :  119.75 seconds

PASSED :  5 / 19
[36m[[[sanic-1334]]][0m
[[[ Node ]]]
for bp in chain(blueprints):
    if isinstance(bp.url_prefix, type(None)):
        bp.url_prefix = ''
    bp.url_prefix = url_prefix + bp.url_prefix
    bps.append(bp)
[32mPASSED![0m
Time :  1.96 seconds

[36m[[[sanic-2008-1]]][0m
[[[ Node ]]]
if not path.isfile(str(file_or_directory)):
    uri += '<file_uri:' + pattern + '>'
[32mPASSED![0m
Time :  3.79 seconds

[36m[[[sanic-2008-2]]][0m
[[[ Node ]]]
async def _static_request_handler(file_or_directory, use_modified_since, use_content_range, stream_large_files, request, content_type=None, file_uri=None):
    import pathlib
    if isinstance(file_or_directory, bytes):
        file_or_directory = str(file_or_directory, 'utf-8')
    elif isinstance(file_or_directory, pathlib.PosixPath):
        file_or_directory = str(file_or_directory)
    if file_uri and '../' in file_uri:
        raise InvalidUsage('Invalid URL')
    root_path = file_path = file_or_directory
    if file_uri:
        file_path = path.join(file_or_directory, sub('^[/]*', '', file_uri))
    file_path = path.abspath(unquote(file_path))
    if not file_path.startswith(path.abspath(unquote(root_path))):
        error_logger.exception(f'File not found: path={file_or_directory}, relative_url={file_uri}')
        raise FileNotFound('File not found', path=file_or_directory, relative_url=file_uri)
    try:
        headers = {}
        stats = None
        if use_modified_since:
            stats = await stat_async(file_path)
            modified_since = strftime('%a, %d %b %Y %H:%M:%S GMT', gmtime(stats.st_mtime))
            if request.headers.get('If-Modified-Since') == modified_since:
                return HTTPResponse(status=304)
            headers['Last-Modified'] = modified_since
        _range = None
        if use_content_range:
            _range = None
            if not stats:
                stats = await stat_async(file_path)
            headers['Accept-Ranges'] = 'bytes'
            headers['Content-Length'] = str(stats.st_size)
            if request.method != 'HEAD':
                try:
                    _range = ContentRangeHandler(request, stats)
                except HeaderNotFound:
                    pass
                else:
                    del headers['Content-Length']
                    for (key, value) in _range.headers.items():
                        headers[key] = value
        headers['Content-Type'] = content_type or guess_type(file_path)[0] or 'text/plain'
        if request.method == 'HEAD':
            return HTTPResponse(headers=headers)
        else:
            if stream_large_files:
                if type(stream_large_files) == int:
                    threshold = stream_large_files
                else:
                    threshold = 1024 * 1024
                if not stats:
                    stats = await stat_async(file_path)
                if stats.st_size >= threshold:
                    return await file_stream(file_path, headers=headers, _range=_range)
            return await file(file_path, headers=headers, _range=_range)
    except ContentRangeError:
        raise
    except Exception:
        error_logger.exception(f'File not found: path={file_or_directory}, relative_url={file_uri}')
        raise FileNotFound('File not found', path=file_or_directory, relative_url=file_uri)
[32mPASSED![0m
Time :  21.75 seconds

PASSED :  3 / 6
[36m[[[scikitlearn-7064]]][0m
[[[ Node ]]]
def fit(self, X, y, sample_weight=None):
    """Fit the SVM model according to the given training data.

        Parameters
        ----------
        X : {array-like, sparse matrix}, shape (n_samples, n_features)
            Training vectors, where n_samples is the number of samples
            and n_features is the number of features.
            For kernel="precomputed", the expected shape of X is
            (n_samples, n_samples).

        y : array-like, shape (n_samples,)
            Target values (class labels in classification, real numbers in
            regression)

        sample_weight : array-like, shape (n_samples,)
            Per-sample weights. Rescale C per sample. Higher weights
            force the classifier to put more emphasis on these points.

        Returns
        -------
        self : object
            Returns self.

        Notes
        ------
        If X and y are not C-ordered and contiguous arrays of np.float64 and
        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.

        If X is a dense array, then the other methods will not support sparse
        matrices as input.
        """
    rnd = check_random_state(self.random_state)
    sparse = sp.isspmatrix(X)
    if sparse and self.kernel == 'precomputed':
        raise TypeError('Sparse precomputed kernels are not supported.')
    self._sparse = sparse and (not callable(self.kernel))
    (X, y) = check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr')
    y = self._validate_targets(y)
    sample_weight = np.asarray([] if sample_weight is None else sample_weight, dtype=np.float64)
    solver_type = LIBSVM_IMPL.index(self._impl)
    if solver_type != 2 and X.shape[0] != y.shape[0]:
        raise ValueError('X and y have incompatible shapes.\n' + 'X has %s samples, but y has %s.' % (X.shape[0], y.shape[0]))
    if self.kernel == 'precomputed' and X.shape[0] != X.shape[1]:
        raise ValueError('X.shape[0] should be equal to X.shape[1]')
    if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:
        raise ValueError('sample_weight and X have incompatible shapes: %r vs %r\nNote: Sparse matrices cannot be indexed w/boolean masks (use `indices=True` in CV).' % (sample_weight.shape, X.shape))
    if self.gamma == 'auto':
        self._gamma = 1.0 / X.shape[1]
    else:
        self._gamma = self.gamma
    kernel = self.kernel
    if callable(kernel):
        kernel = 'precomputed'
    fit = self._sparse_fit if self._sparse else self._dense_fit
    if self.verbose:
        print('[LibSVM]', end='')
    seed = rnd.randint(np.iinfo('i').max)
    if isinstance(kernel, bytes):
        kernel = str(kernel, 'utf-8')
    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)
    self.shape_fit_ = X.shape
    self._intercept_ = self.intercept_.copy()
    self._dual_coef_ = self.dual_coef_
    if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2:
        self.intercept_ *= -1
        self.dual_coef_ = -self.dual_coef_
    return self
[32mPASSED![0m
Time :  3.8 seconds

[36m[[[scikitlearn-8973]]][0m
[[[ Node ]]]
folds = list(cv.split(X, y=y))
[32mPASSED![0m
Time :  3.04 seconds

PASSED :  2 / 7
[36m[[[tornado-1689]]][0m
[[[ Node ]]]
if not _time_independent_equals(utf8(b'' if token is None else token), utf8(expected_token)):
    raise HTTPError(403, 'XSRF cookie does not match POST argument')
[32mPASSED![0m
Time :  4.63 seconds

PASSED :  1 / 2
PASSED :  0 / 1
[36m[[[Zappa-388]]][0m
[[[ Node ]]]
environ['CONTENT_LENGTH'] = str(len(unicode() if body is None else body))
[32mPASSED![0m
Time :  152.13 seconds

PASSED :  1 / 4
Total :  37 / 157
