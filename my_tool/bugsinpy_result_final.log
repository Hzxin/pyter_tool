[36m[[[ansible-1]]][0m
[31mFAILED...[0m
Time :  183.81 seconds

PASSED :  0 / 1
[36m[[[fastapi-1]]][0m
/home/wonseok/BugsInPy/benchmark/fastapi-1/tests/test_jsonable_encoder.py not exists
[31mFAILED...[0m
Time :  0.0 seconds

[36m[[[fastapi-7]]][0m
[31mFAILED...[0m
Time :  6.78 seconds

[36m[[[fastapi-11]]][0m
[31mFAILED...[0m
Time :  38.48 seconds

PASSED :  0 / 3
[36m[[[keras-4]]][0m
[[[ Node ]]]
@interfaces.legacy_get_updates_support
def get_updates(self, loss, params):
    if isinstance(params, list):
        return params
    grads = self.optimizer.compute_gradients(loss, params)
    self.updates = [K.update_add(self.iterations, 1)]
    opt_update = self.optimizer.apply_gradients(grads, global_step=self.iterations)
    self.updates.append(opt_update)
    return self.updates
[32mPASSED![0m
Time :  55.86 seconds

[36m[[[keras-21]]][0m
/home/wonseok/BugsInPy/benchmark/keras-21/tests/keras/test_callbacks.py not exists
[31mFAILED...[0m
Time :  0.02 seconds

[36m[[[keras-22]]][0m
Timeout!
Time :  3600.01 seconds

[36m[[[keras-34]]][0m
[[[ Node ]]]
while steps_done < steps_per_epoch:
    import keras
    if issubclass(output_generator.__class__, keras.utils.data_utils.Sequence):
        return self.history
    generator_output = next(output_generator)
    if not hasattr(generator_output, '__len__'):
        raise ValueError('Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: ' + str(generator_output))
    if len(generator_output) == 2:
        (x, y) = generator_output
        sample_weight = None
    elif len(generator_output) == 3:
        (x, y, sample_weight) = generator_output
    else:
        raise ValueError('Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: ' + str(generator_output))
    batch_logs = {}
    if isinstance(x, list):
        batch_size = x[0].shape[0]
    elif isinstance(x, dict):
        batch_size = list(x.values())[0].shape[0]
    else:
        batch_size = x.shape[0]
    batch_logs['batch'] = batch_index
    batch_logs['size'] = batch_size
    callbacks.on_batch_begin(batch_index, batch_logs)
    outs = self.train_on_batch(x, y, sample_weight=sample_weight, class_weight=class_weight)
    if not isinstance(outs, list):
        outs = [outs]
    for (l, o) in zip(out_labels, outs):
        batch_logs[l] = o
    callbacks.on_batch_end(batch_index, batch_logs)
    batch_index += 1
    steps_done += 1
    if steps_done >= steps_per_epoch and do_validation:
        if val_gen:
            val_outs = self.evaluate_generator(validation_generator, validation_steps, workers=0)
        else:
            val_outs = self.evaluate(val_x, val_y, batch_size=batch_size, sample_weight=val_sample_weights, verbose=0)
        if not isinstance(val_outs, list):
            val_outs = [val_outs]
        for (l, o) in zip(out_labels, val_outs):
            epoch_logs['val_' + l] = o
    if callback_model.stop_training:
        break
[32mPASSED![0m
Time :  323.74 seconds

[36m[[[keras-39]]][0m
[[[ Node ]]]
if isinstance(self.target, int) and (not force and now - self.last_update < self.interval and (current < self.target)):
    return
[32mPASSED![0m
Time :  7.76 seconds

PASSED :  3 / 5
[36m[[[luigi-3]]][0m
[[[ Node ]]]
return tuple((x if isinstance(x, int) else tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict)))
[32mPASSED![0m
Time :  58.93 seconds

[36m[[[luigi-4]]][0m
[[[ Node ]]]
def copy(self, cursor, f):
    """
        Defines copying from s3 into redshift.

        If both key-based and role-based credentials are provided, role-based will be used.
        """
    logger.info('Inserting file: %s', f)
    colnames = ''
    if self.columns and len(self.columns) > 0:
        colnames = ','.join([x[0] for x in self.columns])
        colnames = '({})'.format(colnames)
    cursor.execute("\n         COPY {table} {colnames} from '{source}'\n         CREDENTIALS '{creds}'\n         {options}\n         ;".format(table=self.table, colnames=colnames, source=f, creds=self._credentials(), options=self.copy_options))
[32mPASSED![0m
Time :  74.88 seconds

[36m[[[luigi-6]]][0m
[31mFAILED...[0m
Time :  94.72 seconds

[36m[[[luigi-14]]][0m
[[[ Node ]]]
def has_excessive_failures(self):
    if self.failures.first_failure_time is not None and self.disable_hard_timeout:
        if time.time() >= self.failures.first_failure_time + self.disable_hard_timeout:
            return True
    if self.disable_failures and self.failures.num_failures() >= self.disable_failures:
        return True
    return False
[32mPASSED![0m
Time :  4.05 seconds

[36m[[[luigi-22]]][0m
[[[ Node ]]]
if isinstance(self.last_active, int) and self.last_active + config.worker_disconnect_delay < time.time():
    return True
[32mPASSED![0m
Time :  4.19 seconds

[36m[[[luigi-25]]][0m
[[[ Node ]]]
path = self.s3_load_path
[32mPASSED![0m
Time :  2.97 seconds

[36m[[[luigi-26]]][0m
[[[ Node ]]]
if not job.jar() or not os.path.exists(job.jar()):
    if not isinstance(job.jar(), type(None)):
        logger.error("Can't find jar: %s, full path %s", job.jar(), os.path.abspath(job.jar()))
    raise HadoopJarJobError('job jar does not exist')
[32mPASSED![0m
Time :  38.64 seconds

PASSED :  6 / 7
[36m[[[matplotlib-15]]][0m
/home/wonseok/BugsInPy/benchmark/matplotlib-15/lib/matplotlib/tests/test_colors.py not exists
[31mFAILED...[0m
Time :  0.0 seconds

PASSED :  0 / 1
[36m[[[pandas-12]]][0m
[31mFAILED...[0m
Time :  227.62 seconds

[36m[[[pandas-13]]][0m
[31mFAILED...[0m
Time :  1369.03 seconds

[36m[[[pandas-30]]][0m
[[[ Node ]]]
try:
    new_data = to_datetime(new_data, errors='raise', unit=date_unit)
except (ValueError, OverflowError):
    continue
except TypeError:
    continue
[32mPASSED![0m
Time :  11.07 seconds

[36m[[[pandas-31]]][0m
[31mFAILED...[0m
Time :  2756.32 seconds

[36m[[[pandas-36]]][0m
[[[ Node ]]]
if isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
    import numpy
    if not (isinstance(obj, numpy.ndarray) and numpy.issubdtype(obj.dtype, numpy.float)):
        return obj.isna()
    return _isna_ndarraylike_old(obj)
elif isinstance(obj, ABCDataFrame):
    return obj.isna()
elif isinstance(obj, list):
    return _isna_ndarraylike_old(np.asarray(obj, dtype=object))
elif hasattr(obj, '__array__'):
    return _isna_ndarraylike_old(np.asarray(obj))
else:
    return obj is None
[32mPASSED![0m
Time :  66.04 seconds

[36m[[[pandas-48]]][0m
[[[ Node ]]]
try:
    result = type(block.values)._from_sequence(result.ravel(), dtype=block.values.dtype)
except ValueError:
    result = result.reshape(1, -1)
except TypeError:
    result = result.reshape(1, -1)
[32mPASSED![0m
Time :  59.91 seconds

[36m[[[pandas-49]]][0m
[[[ Node ]]]
def rep(x, r):
    import pandas
    if isinstance(x, pandas._libs.missing.NAType):
        return x
    try:
        return bytes.__mul__(x, r)
    except TypeError:
        return str.__mul__(x, r)
[32mPASSED![0m
Time :  19.92 seconds

[36m[[[pandas-51]]][0m
[[[ Node ]]]
if self.is_monotonic and other.is_monotonic:
    import pandas
    if not isinstance(other, pandas.core.indexes.category.CategoricalIndex):
        return self._join_monotonic(other, how=how, return_indexers=return_indexers)
else:
    return self._join_non_unique(other, how=how, return_indexers=return_indexers)
[32mPASSED![0m
Time :  34.89 seconds

[36m[[[pandas-53]]][0m
[[[ Node ]]]
if not self.holds_integer():
    if isinstance(key, int):
        return 0
    self._invalid_indexer('label', key)
[32mPASSED![0m
Time :  42.68 seconds

[36m[[[pandas-57]]][0m
/home/wonseok/BugsInPy/benchmark/pandas-57/pandas/tests/arrays/categorical/test_replace.py not exists
/home/wonseok/BugsInPy/benchmark/pandas-57/pandas/tests/arrays/categorical/test_replace.py not exists
inference_typ_dict :  {}
[33mERROR...[0m
  File "/home/wonseok/pyfix/my_tool/test_main.py", line 227, in run
    works.work()
  File "/home/wonseok/pyfix/my_tool/work.py", line 666, in work
    self.patch_only_once(synthe)
  File "/home/wonseok/pyfix/my_tool/work.py", line 530, in patch_only_once
    self.spec_inference(synthe)
  File "/home/wonseok/pyfix/my_tool/work.py", line 264, in spec_inference
    var_score, operator_mutate = error_analysis.extract_score(error_stmt)
  File "/home/wonseok/pyfix/my_tool/type_analysis/error_analysis.py", line 484, in extract_score
    self.visit(node)
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/ast.py", line 407, in visit
    return visitor(node)
  File "/home/wonseok/pyfix/my_tool/type_analysis/error_analysis.py", line 160, in visit_If
    self.visit(node.test)
  File "/home/wonseok/.pyenv/versions/3.9.1/lib/python3.9/ast.py", line 407, in visit
    return visitor(node)
  File "/home/wonseok/pyfix/my_tool/type_analysis/error_analysis.py", line 272, in visit_Compare
    inference_typ = list(typ_dict.keys())[0] # 1st key
list index out of range
Time :  831.67 seconds

[36m[[[pandas-71]]][0m
[31mFAILED...[0m
Time :  2330.85 seconds

[36m[[[pandas-72]]][0m
Timeout!
Time :  3600.01 seconds

[36m[[[pandas-79]]][0m
Timeout!
Time :  3600.04 seconds

[36m[[[pandas-99]]][0m
Timeout!
Time :  3600.04 seconds

[36m[[[pandas-103]]][0m
[31mFAILED...[0m
Time :  340.97 seconds

[36m[[[pandas-106]]][0m
[[[ Node ]]]
@Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)
def get_indexer_non_unique(self, target):
    target = ensure_index(target)
    (pself, ptarget) = self._maybe_promote(target)
    if pself is not self or ptarget is not target:
        return pself.get_indexer_non_unique(ptarget)
    if is_categorical(target):
        tgt_values = np.asarray(target)
    elif self.is_all_dates:
        tgt_values = target.asi8
    else:
        tgt_values = target._ndarray_values
    import pandas
    if isinstance(self, pandas.core.indexes.datetimes.DatetimeIndex):
        raise KeyError('not found in axis')
    (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)
    return (ensure_platform_int(indexer), missing)
[32mPASSED![0m
Time :  2728.25 seconds

[36m[[[pandas-110]]][0m
Timeout!
Time :  3600.07 seconds

[36m[[[pandas-111]]][0m
Timeout!
Time :  3600.08 seconds

[36m[[[pandas-112]]][0m
[[[ Node ]]]
if not is_object_dtype(target_as_index):
    target_as_index = self._maybe_convert_i8(target_as_index)
    import pandas
    if isinstance(target_as_index, pandas.core.indexes.category.CategoricalIndex):
        return np.arange(len(self), dtype='intp')
    indexer = self._engine.get_indexer(target_as_index.values)
else:
    indexer = []
    for key in target_as_index:
        try:
            loc = self.get_loc(key)
        except KeyError:
            loc = -1
        indexer.append(loc)
[32mPASSED![0m
Time :  343.32 seconds

[36m[[[pandas-123]]][0m
Timeout!
Time :  3600.06 seconds

[36m[[[pandas-138]]][0m
[[[ Node ]]]
def quantile(x, q, interpolation_method='fraction'):
    """
    Compute sample quantile or quantiles of the input array. For example, q=0.5
    computes the median.

    The `interpolation_method` parameter supports three values, namely
    `fraction` (default), `lower` and `higher`. Interpolation is done only,
    if the desired quantile lies between two data points `i` and `j`. For
    `fraction`, the result is an interpolated value between `i` and `j`;
    for `lower`, the result is `i`, for `higher` the result is `j`.

    Parameters
    ----------
    x : ndarray
        Values from which to extract score.
    q : scalar or array
        Percentile at which to extract score.
    interpolation_method : {'fraction', 'lower', 'higher'}, optional
        This optional parameter specifies the interpolation method to use,
        when the desired quantile lies between two data points `i` and `j`:

        - fraction: `i + (j - i)*fraction`, where `fraction` is the
                    fractional part of the index surrounded by `i` and `j`.
        -lower: `i`.
        - higher: `j`.

    Returns
    -------
    score : float
        Score at percentile.

    Examples
    --------
    >>> from scipy import stats
    >>> a = np.arange(100)
    >>> stats.scoreatpercentile(a, 50)
    49.5

    """
    x = np.asarray(x)
    mask = isna(x)
    x = x[~mask]
    import numpy
    if not (numpy.issubdtype(x.dtype, numpy.number)):
        x = x.astype(numpy.number)
    values = np.sort(x)

    def _interpolate(a, b, fraction):
        """Returns the point at the given fraction between a and b, where
        'fraction' must be between 0 and 1.
        """
        return a + (b - a) * fraction

    def _get_score(at):
        if len(values) == 0:
            return np.nan
        idx = at * (len(values) - 1)
        if idx % 1 == 0:
            score = values[int(idx)]
        elif interpolation_method == 'fraction':
            score = _interpolate(values[int(idx)], values[int(idx) + 1], idx % 1)
        elif interpolation_method == 'lower':
            score = values[np.floor(idx)]
        elif interpolation_method == 'higher':
            score = values[np.ceil(idx)]
        else:
            raise ValueError("interpolation_method can only be 'fraction' , 'lower' or 'higher'")
        return score
    if is_scalar(q):
        return _get_score(q)
    else:
        q = np.asarray(q, np.float64)
        result = [_get_score(x) for x in q]
        result = np.array(result, dtype=np.float64)
        return result
[32mPASSED![0m
Time :  131.85 seconds

[36m[[[pandas-142]]][0m
[[[ Node ]]]
if is_timedelta:
    res = arr[res_indexer]
    lag = arr[lag_indexer]
    mask = (arr[res_indexer] == na) | (arr[lag_indexer] == na)
    if mask.any():
        res = res.copy()
        res[mask] = 0
        lag = lag.copy()
        lag[mask] = 0
    result = res - lag
    result[mask] = na
    out_arr[res_indexer] = result
else:
    import numpy
    if isinstance(arr, numpy.ndarray) and arr.dtype.type is numpy.bool_:
        out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]
    else:
        out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]
[32mPASSED![0m
Time :  771.78 seconds

[36m[[[pandas-145]]][0m
Timeout!
Time :  3600.15 seconds

[36m[[[pandas-146]]][0m
[[[ Node ]]]
def array_equivalent(left, right, strict_nan=False):
    """
    True if two arrays, left and right, have equal non-NaN elements, and NaNs
    in corresponding locations.  False otherwise. It is assumed that left and
    right are NumPy arrays of the same dtype. The behavior of this function
    (particularly with respect to NaNs) is not defined if the dtypes are
    different.

    Parameters
    ----------
    left, right : ndarrays
    strict_nan : bool, default False
        If True, consider NaN and None to be different.

    Returns
    -------
    b : bool
        Returns True if the arrays are equivalent.

    Examples
    --------
    >>> array_equivalent(
    ...     np.array([1, 2, np.nan]),
    ...     np.array([1, 2, np.nan]))
    True
    >>> array_equivalent(
    ...     np.array([1, np.nan, 2]),
    ...     np.array([1, 2, np.nan]))
    False
    """
    (left, right) = (np.asarray(left), np.asarray(right))
    if left.shape != right.shape:
        return False
    try:
        if is_string_dtype(left) or is_string_dtype(right):
            if not strict_nan:
                return lib.array_equivalent_object(ensure_object(left.ravel()), ensure_object(right.ravel()))
            for (left_value, right_value) in zip(left, right):
                if left_value is NaT and right_value is not NaT:
                    return False
                elif isinstance(left_value, float) and np.isnan(left_value):
                    if not isinstance(right_value, float) or not np.isnan(right_value):
                        return False
                elif np.any(left_value != right_value):
                    return False
            return True
    except TypeError:
        return ((left == right) | isna(left) & isna(right)).all()
    if is_float_dtype(left) or is_complex_dtype(left):
        if not (np.prod(left.shape) and np.prod(right.shape)):
            return True
        return ((left == right) | isna(left) & isna(right)).all()
    elif is_datetimelike_v_numeric(left, right):
        return False
    elif needs_i8_conversion(left) and needs_i8_conversion(right):
        if not is_dtype_equal(left.dtype, right.dtype):
            return False
        left = left.view('i8')
        right = right.view('i8')
    if left.dtype.type is np.void or right.dtype.type is np.void:
        if left.dtype != right.dtype:
            return False
    return np.array_equal(left, right)
[32mPASSED![0m
Time :  551.5 seconds

[36m[[[pandas-152]]][0m
[[[ Node ]]]
to_concat = [self] + list(to_append)
[32mPASSED![0m
Time :  21.66 seconds

[36m[[[pandas-158]]][0m
Timeout!
Time :  3600.05 seconds

PASSED :  12 / 26
[36m[[[scrapy-1]]][0m
[[[ Node ]]]
for domain in allowed_domains:
    if domain and url_pattern.match(domain):
        message = 'allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains.' % domain
        warnings.warn(message, URLWarning)
[32mPASSED![0m
Time :  2.24 seconds

[36m[[[scrapy-2]]][0m
[[[ Node ]]]
def __setitem__(self, key, value):
    if not isinstance(self.limit, type(None)):
        while len(self) >= self.limit:
            self.popitem(last=False)
    super(LocalCache, self).__setitem__(key, value)
[32mPASSED![0m
Time :  3.77 seconds

[36m[[[scrapy-8]]][0m
/home/wonseok/BugsInPy/benchmark/scrapy-8/tests/test_item.py not exists
/home/wonseok/BugsInPy/benchmark/scrapy-8/tests/test_item.py not exists
[31mFAILED...[0m
Time :  0.0 seconds

[36m[[[scrapy-17]]][0m
[[[ Node ]]]
return '%s %s' % (status, to_native_str('Unknown Status' if isinstance(http.RESPONSES.get(int(status)), type(None)) else http.RESPONSES.get(int(status))))
[32mPASSED![0m
Time :  141.86 seconds

[36m[[[scrapy-20]]][0m
[31mFAILED...[0m
Time :  33.49 seconds

[36m[[[scrapy-23]]][0m
[31mFAILED...[0m
Time :  61.77 seconds

[36m[[[scrapy-27]]][0m
[[[ Node ]]]
if response.status in [301, 307] and 'Location' in response.headers:
    import scrapy
    if isinstance(request, scrapy.http.request.Request):
        return response
    redirected_url = urljoin(request.url, response.headers['location'])
    redirected = request.replace(url=redirected_url)
    return self._redirect(redirected, request, spider, response.status)
[32mPASSED![0m
Time :  4.38 seconds

[36m[[[scrapy-29]]][0m
[[[ Node ]]]
s += b'Host: ' + to_bytes('' if isinstance(parsed.hostname, type(None)) else parsed.hostname) + b'\r\n'
[32mPASSED![0m
Time :  3.24 seconds

[36m[[[scrapy-30]]][0m
/home/wonseok/BugsInPy/benchmark/scrapy-30/tests/test_command_version.py not exists
/home/wonseok/BugsInPy/benchmark/scrapy-30/tests/test_command_version.py not exists
[31mFAILED...[0m
Time :  21.11 seconds

[36m[[[scrapy-40]]][0m
[[[ Node ]]]
if self.binary:
    return to_bytes(value, encoding=self.encoding)
else:
    if not (isinstance(value, str)):
        return value
    return to_unicode(value, encoding=self.encoding)
[32mPASSED![0m
Time :  20.23 seconds

PASSED :  6 / 10
[36m[[[spacy-5]]][0m
[[[ Node ]]]
def evaluate(self, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None):
    """Evaluate a model's pipeline components.

        docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.
        verbose (bool): Print debugging information.
        batch_size (int): Batch size to use.
        scorer (Scorer): Optional `Scorer` to use. If not passed in, a new one
            will be created.
        component_cfg (dict): An optional dictionary with extra keyword
            arguments for specific components.
        RETURNS (Scorer): The scorer containing the evaluation results.

        DOCS: https://spacy.io/api/language#evaluate
        """
    if scorer is None:
        scorer = Scorer(pipeline=self.pipeline)
    if component_cfg is None:
        component_cfg = {}
    (docs, golds) = zip(*docs_golds)
    docs = [self.make_doc(doc) if isinstance(doc, basestring_) else doc for doc in docs]
    golds = list(golds)
    for (name, pipe) in self.pipeline:
        kwargs = component_cfg.get(name, {})
        kwargs.setdefault('batch_size', batch_size)
        if not hasattr(pipe, 'pipe'):
            docs = _pipe(pipe, docs, kwargs)
        else:
            docs = pipe.pipe(docs, **kwargs)
    if isinstance(golds, list):
        return scorer
    for (doc, gold) in zip(docs, golds):
        if not isinstance(gold, GoldParse):
            gold = GoldParse(doc, **gold)
        if verbose:
            print(doc)
        kwargs = component_cfg.get('scorer', {})
        kwargs.setdefault('verbose', verbose)
        scorer.score(doc, gold, **kwargs)
    return scorer
[32mPASSED![0m
Time :  16.83 seconds

PASSED :  1 / 1
[36m[[[tornado-7]]][0m
/home/wonseok/BugsInPy/benchmark/tornado-7/tornado/gen.py not exists
[33mERROR...[0m
  File "/home/wonseok/pyfix/my_tool/test_main.py", line 227, in run
    works.work()
  File "/home/wonseok/pyfix/my_tool/work.py", line 807, in work
    neg_file_node = self.files[neg_filename]
'/home/wonseok/BugsInPy/benchmark/tornado-7/tornado/gen.py'
Time :  0.01 seconds

[36m[[[tornado-9]]][0m
[[[ Node ]]]
def url_concat(url, args):
    if isinstance(args, type(None)):
        return url
    'Concatenate url and arguments regardless of whether\n    url has existing query parameters.\n\n    ``args`` may be either a dictionary or a list of key-value pairs\n    (the latter allows for multiple values with the same key.\n\n    >>> url_concat("http://example.com/foo", dict(c="d"))\n    \'http://example.com/foo?c=d\'\n    >>> url_concat("http://example.com/foo?a=b", dict(c="d"))\n    \'http://example.com/foo?a=b&c=d\'\n    >>> url_concat("http://example.com/foo?a=b", [("c", "d"), ("c", "d2")])\n    \'http://example.com/foo?a=b&c=d&c=d2\'\n    '
    parsed_url = urlparse(url)
    if isinstance(args, dict):
        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
        parsed_query.extend(args.items())
    elif isinstance(args, list) or isinstance(args, tuple):
        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
        parsed_query.extend(args)
    else:
        err = "'args' parameter should be dict, list or tuple. Not {0}".format(type(args))
        raise TypeError(err)
    final_query = urlencode(parsed_query)
    url = urlunparse((parsed_url[0], parsed_url[1], parsed_url[2], parsed_url[3], final_query, parsed_url[5]))
    return url
[32mPASSED![0m
Time :  1.56 seconds

PASSED :  1 / 2
[36m[[[tqdm-9]]][0m
None Type Casting Other Type
Type :  range
None Type Casting Other Type
Type :  range
None Type Casting Other Type
Type :  range
None Type Casting Other Type
Type :  range
[31mFAILED...[0m
Time :  6.35 seconds

PASSED :  0 / 1
[36m[[[youtubedl-11]]][0m
[[[ Node ]]]
def str_to_int(int_str):
    """ A more relaxed version of int_or_none """
    if int_str is None:
        return None
    if isinstance(int_str, int):
        int_str = str(int_str)
    int_str = re.sub('[,\\.\\+]', '', int_str)
    return int(int_str)
[32mPASSED![0m
Time :  2.9 seconds

[36m[[[youtubedl-16]]][0m
[31mFAILED...[0m
Time :  1789.44 seconds

PASSED :  1 / 2
Total :  30 / 59
